{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/jovyan/.keras’: File exists\n",
      "cp: cannot stat 'keras.json': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir $HOME/.keras\n",
    "cp keras.json $HOME/.keras/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "device_name = tf.test.gpu_device_name()\n",
    "#keras.backend.image_data_format()\n",
    "keras.__version__\n",
    "tf.__version__\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import hyperopt.fmin as hypfmin\n",
    "import keras\n",
    "import theano\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "# from\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.optimizers import Adam\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 2 1.2.2\n"
     ]
    }
   ],
   "source": [
    "keras.__version__\n",
    "ε = 1\n",
    "σ = 2\n",
    "print(ε,σ,σ,keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GpuElemwise{exp,no_inplace}(<GpuArrayType<None>(float32, vector)>), HostFromGpu(gpuarray)(GpuElemwise{exp,no_inplace}.0)]\n",
      "Looping 100000 times took 29.606136 seconds\n",
      "Result is [1.2317803 1.6187935 1.5227807 ... 2.2077181 2.2996776 1.623233 ]\n",
      "Used the gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from theano import function, config, shared, tensor\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 100000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], tensor.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in range(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, tensor.Elemwise) and\n",
    "              ('Gpu' not in type(x.op).__name__)\n",
    "              for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyro-ppl\n",
      "  Downloading pyro-ppl-0.1.2.tar.gz (71kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 1.2MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /opt/conda/lib/python3.6/site-packages (from pyro-ppl)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.6/site-packages (from pyro-ppl)\n",
      "Collecting cloudpickle>=0.3.1 (from pyro-ppl)\n",
      "  Downloading cloudpickle-0.5.2-py2.py3-none-any.whl\n",
      "Collecting graphviz>=0.8 (from pyro-ppl)\n",
      "  Downloading graphviz-0.8.2-py2.py3-none-any.whl\n",
      "Collecting networkx>=2.0.0 (from pyro-ppl)\n",
      "  Downloading networkx-2.1.zip (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 238kB/s ta 0:00:01    94% |██████████████████████████████▎ | 1.5MB 6.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting observations>=0.1.4 (from pyro-ppl)\n",
      "  Downloading observations-0.1.4.tar.gz (483kB)\n",
      "\u001b[K    100% |████████████████████████████████| 491kB 835kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from pyro-ppl)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from pyro-ppl)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0.0->pyro-ppl)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from torch->pyro-ppl)\n",
      "Building wheels for collected packages: pyro-ppl, networkx, observations\n",
      "  Running setup.py bdist_wheel for pyro-ppl ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/51/fb/ba/0ff962e95a4f61025ff5262bd9f36c983e728db3bc1bb1cbd4\n",
      "  Running setup.py bdist_wheel for networkx ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/a0/33/4e/7c9228ea77f8090e895d8d2b76f3b5a76997a5b3edeb4e2c6f\n",
      "  Running setup.py bdist_wheel for observations ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/8b/b7/74/106ea929310039388cf7ddbddd40fe3d0b77af36e51405f653\n",
      "Successfully built pyro-ppl networkx observations\n",
      "Installing collected packages: cloudpickle, graphviz, networkx, observations, pyro-ppl\n",
      "  Found existing installation: networkx 1.11\n",
      "    Uninstalling networkx-1.11:\n",
      "      Successfully uninstalled networkx-1.11\n",
      "Successfully installed cloudpickle-0.5.2 graphviz-0.8.2 networkx-2.1 observations-0.1.4 pyro-ppl-0.1.2\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyro-ppl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.718062162399292\n",
      "1.7174491882324219\n",
      "1.7187464237213135\n",
      "1.7174005508422852\n",
      "1.7173093557357788\n",
      "1.7184616327285767\n",
      "1.7190818786621094\n",
      "1.7198518514633179\n",
      "1.7194294929504395\n",
      "1.7181050777435303\n",
      "1.7170093059539795\n",
      "1.7199640274047852\n",
      "1.7184996604919434\n",
      "1.7193413972854614\n",
      "1.7193055152893066\n",
      "1.7189314365386963\n",
      "1.7171235084533691\n",
      "1.7179014682769775\n",
      "1.7187113761901855\n",
      "1.7167631387710571\n",
      "1.7184247970581055\n",
      "1.7195534706115723\n",
      "1.7174320220947266\n",
      "1.7181025743484497\n",
      "1.718043565750122\n",
      "1.7172857522964478\n",
      "1.7185003757476807\n",
      "1.71705162525177\n",
      "1.7187999486923218\n",
      "1.716813564300537\n",
      "1.7191725969314575\n",
      "1.7183152437210083\n",
      "1.7194275856018066\n",
      "1.7185500860214233\n",
      "1.717462182044983\n",
      "1.7193547487258911\n",
      "1.7185266017913818\n",
      "1.7168415784835815\n",
      "1.717861294746399\n",
      "1.715272307395935\n",
      "1.7203388214111328\n",
      "1.7198158502578735\n",
      "1.7201447486877441\n",
      "1.7163861989974976\n",
      "1.7177456617355347\n",
      "1.7182488441467285\n",
      "1.7170679569244385\n",
      "1.7184967994689941\n",
      "1.718663215637207\n",
      "1.718636155128479\n",
      "1.7181655168533325\n",
      "1.7182408571243286\n",
      "1.7168893814086914\n",
      "1.7199618816375732\n",
      "1.7179456949234009\n",
      "1.7184771299362183\n",
      "1.7181576490402222\n",
      "1.7177339792251587\n",
      "1.7175272703170776\n",
      "1.7174017429351807\n",
      "1.7183566093444824\n",
      "1.7185864448547363\n",
      "1.7172185182571411\n",
      "1.7166510820388794\n",
      "1.7186739444732666\n",
      "1.7199734449386597\n",
      "1.719153881072998\n",
      "1.7195605039596558\n",
      "1.7200572490692139\n",
      "1.7175079584121704\n",
      "1.7184343338012695\n",
      "1.7180324792861938\n",
      "1.7158708572387695\n",
      "1.7173560857772827\n",
      "1.7162601947784424\n",
      "1.7189122438430786\n",
      "1.7178454399108887\n",
      "1.7186026573181152\n",
      "1.7174547910690308\n",
      "1.7186318635940552\n",
      "1.717816948890686\n",
      "1.720003366470337\n",
      "1.7192341089248657\n",
      "1.7173664569854736\n",
      "1.7185943126678467\n",
      "1.7190954685211182\n",
      "1.7184293270111084\n",
      "1.7183338403701782\n",
      "1.7194089889526367\n",
      "1.7172701358795166\n",
      "1.7157821655273438\n",
      "1.7182972431182861\n",
      "1.7186747789382935\n",
      "1.7176979780197144\n",
      "1.7180664539337158\n",
      "1.718791127204895\n",
      "1.7167532444000244\n",
      "1.71954345703125\n",
      "1.7176036834716797\n",
      "1.7186866998672485\n",
      "1.7176533937454224\n",
      "1.716916561126709\n",
      "1.7169140577316284\n",
      "1.7195255756378174\n",
      "1.718438982963562\n",
      "1.7184560298919678\n",
      "1.7188001871109009\n",
      "1.715728521347046\n",
      "1.7185815572738647\n",
      "1.7194552421569824\n",
      "1.719303011894226\n",
      "1.7192561626434326\n",
      "1.7180055379867554\n",
      "1.7162880897521973\n",
      "1.716282844543457\n",
      "1.7196096181869507\n",
      "1.7182358503341675\n",
      "1.71879243850708\n",
      "1.7171138525009155\n",
      "1.7170597314834595\n",
      "1.7166459560394287\n",
      "1.7182066440582275\n",
      "1.7182483673095703\n",
      "1.717721939086914\n",
      "1.7185068130493164\n",
      "1.7197012901306152\n",
      "1.720557451248169\n",
      "1.7171624898910522\n",
      "1.7166473865509033\n",
      "1.7182676792144775\n",
      "1.7178502082824707\n",
      "1.7199647426605225\n",
      "1.7186287641525269\n",
      "1.719449520111084\n",
      "1.7180297374725342\n",
      "1.7201801538467407\n",
      "1.7166149616241455\n",
      "1.7186068296432495\n",
      "1.7187014818191528\n",
      "1.7193635702133179\n",
      "1.7205582857131958\n",
      "1.7199081182479858\n",
      "1.7193626165390015\n",
      "1.718395709991455\n",
      "1.7185423374176025\n",
      "1.7164950370788574\n",
      "1.7192522287368774\n",
      "1.7167099714279175\n",
      "1.7198868989944458\n",
      "1.7164312601089478\n",
      "1.7199513912200928\n",
      "1.717655062675476\n",
      "1.7177983522415161\n",
      "1.7180087566375732\n",
      "1.7180334329605103\n",
      "1.7183924913406372\n",
      "1.7181594371795654\n",
      "1.7184536457061768\n",
      "1.7172054052352905\n",
      "1.717929720878601\n",
      "1.7165658473968506\n",
      "1.7187964916229248\n",
      "1.7186169624328613\n",
      "1.7159935235977173\n",
      "1.7176849842071533\n",
      "1.7207379341125488\n",
      "1.7181477546691895\n",
      "1.717225432395935\n",
      "1.7183138132095337\n",
      "1.717058539390564\n",
      "1.7183986902236938\n",
      "1.717158317565918\n",
      "1.7205450534820557\n",
      "1.7179012298583984\n",
      "1.7175182104110718\n",
      "1.7170913219451904\n",
      "1.7167472839355469\n",
      "1.7166553735733032\n",
      "1.7186442613601685\n",
      "1.719154953956604\n",
      "1.7186803817749023\n",
      "1.7181646823883057\n",
      "1.7163375616073608\n",
      "1.7194172143936157\n",
      "1.7193676233291626\n",
      "1.7207874059677124\n",
      "1.7192869186401367\n",
      "1.719104528427124\n",
      "1.7181074619293213\n",
      "1.7180449962615967\n",
      "1.7180510759353638\n",
      "1.7193453311920166\n",
      "1.7165882587432861\n",
      "1.7201459407806396\n",
      "1.7175580263137817\n",
      "1.7168272733688354\n",
      "1.7191054821014404\n",
      "1.718787431716919\n",
      "1.7152926921844482\n",
      "1.7186024188995361\n",
      "1.717820644378662\n",
      "1.7183725833892822\n",
      "1.7176573276519775\n",
      "1.7188985347747803\n",
      "1.7196015119552612\n",
      "1.7157102823257446\n",
      "1.718388557434082\n",
      "1.71772038936615\n",
      "1.7196013927459717\n",
      "1.7178232669830322\n",
      "1.7187663316726685\n",
      "1.718253493309021\n",
      "1.7181168794631958\n",
      "1.717530369758606\n",
      "1.7176947593688965\n",
      "1.718523383140564\n",
      "1.7167645692825317\n",
      "1.7174437046051025\n",
      "1.7182278633117676\n",
      "1.717300534248352\n",
      "1.7188786268234253\n",
      "1.7182271480560303\n",
      "1.7170467376708984\n",
      "1.718174695968628\n",
      "1.719354271888733\n",
      "1.7171902656555176\n",
      "1.7180027961730957\n",
      "1.718060851097107\n",
      "1.7182854413986206\n",
      "1.7191941738128662\n",
      "1.7185370922088623\n",
      "1.7181453704833984\n",
      "1.7176271677017212\n",
      "1.7167832851409912\n",
      "1.7191606760025024\n",
      "1.7177367210388184\n",
      "1.719193935394287\n",
      "1.7179926633834839\n",
      "1.7192994356155396\n",
      "1.7193663120269775\n",
      "1.7187145948410034\n",
      "1.717674970626831\n",
      "1.7169898748397827\n",
      "1.7179145812988281\n",
      "1.7212989330291748\n",
      "1.718339204788208\n",
      "1.719687819480896\n",
      "1.7176517248153687\n",
      "1.7210936546325684\n",
      "1.717256784439087\n",
      "1.7187310457229614\n",
      "1.718916654586792\n",
      "1.718021273612976\n",
      "1.7183170318603516\n",
      "1.717345952987671\n",
      "1.7177906036376953\n",
      "1.7181103229522705\n",
      "1.71746027469635\n",
      "1.7186524868011475\n",
      "1.7180414199829102\n",
      "1.7167401313781738\n",
      "1.7167431116104126\n",
      "1.7171010971069336\n",
      "1.71863853931427\n",
      "1.7183730602264404\n",
      "1.7192877531051636\n",
      "1.7172960042953491\n",
      "1.7179328203201294\n",
      "1.7177647352218628\n",
      "1.719107747077942\n",
      "1.7187923192977905\n",
      "1.7184967994689941\n",
      "1.7182869911193848\n",
      "1.7183620929718018\n",
      "1.7182368040084839\n",
      "1.7187484502792358\n",
      "1.7190395593643188\n",
      "1.7180864810943604\n",
      "1.719264268875122\n",
      "1.7171800136566162\n",
      "1.7178834676742554\n",
      "1.7201687097549438\n",
      "1.716950535774231\n",
      "1.7191778421401978\n",
      "1.7201662063598633\n",
      "1.7196043729782104\n",
      "1.7186968326568604\n",
      "1.7192755937576294\n",
      "1.7195713520050049\n",
      "1.718157410621643\n",
      "1.7172183990478516\n",
      "1.7171001434326172\n",
      "1.7174395322799683\n",
      "1.7166943550109863\n",
      "1.7181813716888428\n",
      "1.7192517518997192\n",
      "1.7184650897979736\n",
      "1.7175252437591553\n",
      "1.7182669639587402\n",
      "1.71645987033844\n",
      "1.7183631658554077\n",
      "1.718834638595581\n",
      "1.7179889678955078\n",
      "1.717631459236145\n",
      "1.718274712562561\n",
      "1.7179646492004395\n",
      "1.717604160308838\n",
      "1.7186567783355713\n",
      "1.7180118560791016\n",
      "1.7174605131149292\n",
      "1.7175348997116089\n",
      "1.717628002166748\n",
      "1.7189933061599731\n",
      "1.719689130783081\n",
      "1.7188761234283447\n",
      "1.7187612056732178\n",
      "1.7172623872756958\n",
      "1.7192355394363403\n",
      "1.7178846597671509\n",
      "1.7191412448883057\n",
      "1.7166268825531006\n",
      "1.7208890914916992\n",
      "1.7188761234283447\n",
      "1.7187365293502808\n",
      "1.7192693948745728\n",
      "1.7174928188323975\n",
      "1.7186741828918457\n",
      "1.7193363904953003\n",
      "1.7192167043685913\n",
      "1.7180474996566772\n",
      "1.718025803565979\n",
      "1.7196402549743652\n",
      "1.7185306549072266\n",
      "1.718385100364685\n",
      "1.7171839475631714\n",
      "1.7186447381973267\n",
      "1.7205922603607178\n",
      "1.717780351638794\n",
      "1.718535304069519\n",
      "1.718009114265442\n",
      "1.7185347080230713\n",
      "1.720630168914795\n",
      "1.7188876867294312\n",
      "1.7186342477798462\n",
      "1.7189027070999146\n",
      "1.718788743019104\n",
      "1.7197340726852417\n",
      "1.719522476196289\n",
      "1.7201937437057495\n",
      "1.71945059299469\n",
      "1.71820068359375\n",
      "1.7176722288131714\n",
      "1.7196619510650635\n",
      "1.7200450897216797\n",
      "1.7195860147476196\n",
      "1.7178956270217896\n",
      "1.7182338237762451\n",
      "1.719740390777588\n",
      "1.7187103033065796\n",
      "1.7191396951675415\n",
      "1.7186121940612793\n",
      "1.719292163848877\n",
      "1.7196664810180664\n",
      "1.7174428701400757\n",
      "1.7169960737228394\n",
      "1.7181751728057861\n",
      "1.7183388471603394\n",
      "1.7158132791519165\n",
      "1.7171329259872437\n",
      "1.7181919813156128\n",
      "1.718416690826416\n",
      "1.7198325395584106\n",
      "1.7164543867111206\n",
      "1.7186099290847778\n",
      "1.7172046899795532\n",
      "1.7191095352172852\n",
      "1.7170498371124268\n",
      "1.717348337173462\n",
      "1.7176787853240967\n",
      "1.7195582389831543\n",
      "1.7176828384399414\n",
      "1.7192233800888062\n",
      "1.7175660133361816\n",
      "1.7174025774002075\n",
      "1.7171491384506226\n",
      "1.7182732820510864\n",
      "1.7174420356750488\n",
      "1.7172666788101196\n",
      "1.7168489694595337\n",
      "1.7181806564331055\n",
      "1.7184350490570068\n",
      "1.7173068523406982\n",
      "1.7181134223937988\n",
      "1.720990538597107\n",
      "1.7193424701690674\n",
      "1.718917727470398\n",
      "1.7188957929611206\n",
      "1.7181003093719482\n",
      "1.7172930240631104\n",
      "1.7167178392410278\n",
      "1.7167738676071167\n",
      "1.717907190322876\n",
      "1.716636061668396\n",
      "1.7191295623779297\n",
      "1.717882513999939\n",
      "1.7170566320419312\n",
      "1.7192903757095337\n",
      "1.7168402671813965\n",
      "1.717751383781433\n",
      "1.718273401260376\n",
      "1.7176944017410278\n",
      "1.7200671434402466\n",
      "1.7182668447494507\n",
      "1.7158446311950684\n",
      "1.7195444107055664\n",
      "1.7181684970855713\n",
      "1.719115138053894\n",
      "1.7182586193084717\n",
      "1.7184371948242188\n",
      "1.7184263467788696\n",
      "1.7197048664093018\n",
      "1.7177015542984009\n",
      "1.7193788290023804\n",
      "1.718398928642273\n",
      "1.717221975326538\n",
      "1.7161904573440552\n",
      "1.7187247276306152\n",
      "1.7174859046936035\n",
      "1.718095064163208\n",
      "1.7189033031463623\n",
      "1.7204489707946777\n",
      "1.7180578708648682\n",
      "1.718714952468872\n",
      "1.7175359725952148\n",
      "1.7192866802215576\n",
      "1.7177865505218506\n",
      "1.716887354850769\n",
      "1.7178863286972046\n",
      "1.7200955152511597\n",
      "1.7179709672927856\n",
      "1.7168421745300293\n",
      "1.7176685333251953\n",
      "1.718194842338562\n",
      "1.719828724861145\n",
      "1.7192696332931519\n",
      "1.7191526889801025\n",
      "1.7179604768753052\n",
      "1.7188541889190674\n",
      "1.7174999713897705\n",
      "1.7189112901687622\n",
      "1.7175064086914062\n",
      "1.717603325843811\n",
      "1.7184096574783325\n",
      "1.717150092124939\n",
      "1.7189089059829712\n",
      "1.7165151834487915\n",
      "1.7185620069503784\n",
      "1.7192096710205078\n",
      "1.7177538871765137\n",
      "1.7187645435333252\n",
      "1.7170861959457397\n",
      "1.717180848121643\n",
      "1.7195614576339722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7165145874023438\n",
      "1.7183250188827515\n",
      "1.7177917957305908\n",
      "1.7176841497421265\n",
      "1.720916986465454\n",
      "1.718488097190857\n",
      "1.7181339263916016\n",
      "1.7162659168243408\n",
      "1.7175390720367432\n",
      "1.71788489818573\n",
      "1.7179234027862549\n",
      "1.7186307907104492\n",
      "1.7158194780349731\n",
      "1.7191158533096313\n",
      "1.720062255859375\n",
      "1.7199342250823975\n",
      "1.717978596687317\n",
      "1.7171002626419067\n",
      "1.7187106609344482\n",
      "1.7181934118270874\n",
      "1.7198289632797241\n",
      "1.718315839767456\n",
      "1.7183233499526978\n",
      "1.7189853191375732\n",
      "1.7187747955322266\n",
      "1.7179166078567505\n",
      "1.7209779024124146\n",
      "1.7164561748504639\n",
      "1.71831214427948\n",
      "1.7166433334350586\n",
      "1.7163903713226318\n",
      "1.7172178030014038\n",
      "1.7174357175827026\n",
      "1.7203782796859741\n",
      "1.719074010848999\n",
      "1.7193197011947632\n",
      "1.7184969186782837\n",
      "1.7173798084259033\n",
      "1.71888267993927\n",
      "1.718077540397644\n",
      "1.7183376550674438\n",
      "1.7192950248718262\n",
      "1.7206518650054932\n",
      "1.7185842990875244\n",
      "1.7163974046707153\n",
      "1.718512773513794\n",
      "1.7173137664794922\n",
      "1.7195842266082764\n",
      "1.7180615663528442\n",
      "1.7186572551727295\n",
      "1.7176264524459839\n",
      "1.718956470489502\n",
      "1.7188657522201538\n",
      "1.7169859409332275\n",
      "1.716729760169983\n",
      "1.7182040214538574\n",
      "1.7173343896865845\n",
      "1.7166013717651367\n",
      "1.7189207077026367\n",
      "1.7183598279953003\n",
      "1.719218373298645\n",
      "1.7185167074203491\n",
      "1.7193361520767212\n",
      "1.719221830368042\n",
      "1.718712329864502\n",
      "1.7181395292282104\n",
      "1.7190030813217163\n",
      "1.7175426483154297\n",
      "1.7182332277297974\n",
      "1.718131184577942\n",
      "1.7177948951721191\n",
      "1.7177728414535522\n",
      "1.7182495594024658\n",
      "1.718124508857727\n",
      "1.717637538909912\n",
      "1.7179189920425415\n",
      "1.7171196937561035\n",
      "1.7206283807754517\n",
      "1.717572569847107\n",
      "1.717706322669983\n",
      "1.7162396907806396\n",
      "1.7187421321868896\n",
      "1.719075083732605\n",
      "1.7186123132705688\n",
      "1.7180469036102295\n",
      "1.7196102142333984\n",
      "1.7174105644226074\n",
      "1.71676504611969\n",
      "1.7174341678619385\n",
      "1.7189159393310547\n",
      "1.7192238569259644\n",
      "1.7192610502243042\n",
      "1.7178845405578613\n",
      "1.7175389528274536\n",
      "1.7180237770080566\n",
      "1.718278408050537\n",
      "1.7163482904434204\n",
      "1.7151362895965576\n",
      "1.7188820838928223\n",
      "1.7171177864074707\n",
      "1.718152403831482\n",
      "1.7185081243515015\n",
      "1.7184913158416748\n",
      "1.7209805250167847\n",
      "1.7184786796569824\n",
      "1.717861294746399\n",
      "1.7185765504837036\n",
      "1.7184932231903076\n",
      "1.7162858247756958\n",
      "1.718505859375\n",
      "1.7162599563598633\n",
      "1.7194589376449585\n",
      "1.7202825546264648\n",
      "1.718628168106079\n",
      "1.718100905418396\n",
      "1.7195266485214233\n",
      "1.7189558744430542\n",
      "1.717637538909912\n",
      "1.7188688516616821\n",
      "1.7179409265518188\n",
      "1.7173948287963867\n",
      "1.7180606126785278\n",
      "1.7183843851089478\n",
      "1.7189817428588867\n",
      "1.717604160308838\n",
      "1.7198889255523682\n",
      "1.7170259952545166\n",
      "1.7184202671051025\n",
      "1.718625783920288\n",
      "1.7183804512023926\n",
      "1.717353105545044\n",
      "1.7203233242034912\n",
      "1.7181998491287231\n",
      "1.7186487913131714\n",
      "1.7179539203643799\n",
      "1.7169357538223267\n",
      "1.719443917274475\n",
      "1.7188599109649658\n",
      "1.7181566953659058\n",
      "1.7189452648162842\n",
      "1.7176475524902344\n",
      "1.7170618772506714\n",
      "1.7185380458831787\n",
      "1.7199420928955078\n",
      "1.718388557434082\n",
      "1.7173744440078735\n",
      "1.7170910835266113\n",
      "1.717361330986023\n",
      "1.7173826694488525\n",
      "1.7187614440917969\n",
      "1.720621109008789\n",
      "1.7176268100738525\n",
      "1.7196440696716309\n",
      "1.7180852890014648\n",
      "1.7191553115844727\n",
      "1.7170910835266113\n",
      "1.7177605628967285\n",
      "1.7182289361953735\n",
      "1.718869686126709\n",
      "1.7178572416305542\n",
      "1.7182399034500122\n",
      "1.7195861339569092\n",
      "1.7164149284362793\n",
      "1.7194056510925293\n",
      "1.7175439596176147\n",
      "1.7191188335418701\n",
      "1.7183996438980103\n",
      "1.7193193435668945\n",
      "1.7182354927062988\n",
      "1.7166434526443481\n",
      "1.718666911125183\n",
      "1.7180571556091309\n",
      "1.7188999652862549\n",
      "1.7169933319091797\n",
      "1.7195537090301514\n",
      "1.7192155122756958\n",
      "1.7187752723693848\n",
      "1.7168453931808472\n",
      "1.7185699939727783\n",
      "1.717352032661438\n",
      "1.7199312448501587\n",
      "1.7188372611999512\n",
      "1.7182834148406982\n",
      "1.7177586555480957\n",
      "1.7179737091064453\n",
      "1.7180228233337402\n",
      "1.7157480716705322\n",
      "1.7192935943603516\n",
      "1.718100905418396\n",
      "1.718082308769226\n",
      "1.7201381921768188\n",
      "1.718116044998169\n",
      "1.7171897888183594\n",
      "1.7183926105499268\n",
      "1.7186098098754883\n",
      "1.7169021368026733\n",
      "1.7181367874145508\n",
      "1.720320224761963\n",
      "1.7175240516662598\n",
      "1.7180801630020142\n",
      "1.7180312871932983\n",
      "1.7184017896652222\n",
      "1.7198859453201294\n",
      "1.718012809753418\n",
      "1.719934344291687\n",
      "1.717899203300476\n",
      "1.717687964439392\n",
      "1.718827486038208\n",
      "1.7163276672363281\n",
      "1.717065453529358\n",
      "1.7173302173614502\n",
      "1.717932105064392\n",
      "1.7180778980255127\n",
      "1.7166996002197266\n",
      "1.7179348468780518\n",
      "1.7183916568756104\n",
      "1.718666672706604\n",
      "1.7168152332305908\n",
      "1.7172794342041016\n",
      "1.7183274030685425\n",
      "1.717345118522644\n",
      "1.7178595066070557\n",
      "1.716103196144104\n",
      "1.7177122831344604\n",
      "1.7185600996017456\n",
      "1.7184240818023682\n",
      "1.7206358909606934\n",
      "1.7183501720428467\n",
      "1.7174030542373657\n",
      "1.718492031097412\n",
      "1.7194052934646606\n",
      "1.7191606760025024\n",
      "1.7194117307662964\n",
      "1.7181628942489624\n",
      "1.719691514968872\n",
      "1.7179523706436157\n",
      "1.7174227237701416\n",
      "1.7187429666519165\n",
      "1.7190643548965454\n",
      "1.7189974784851074\n",
      "1.7187960147857666\n",
      "1.7190548181533813\n",
      "1.7198985815048218\n",
      "1.7196259498596191\n",
      "1.7186777591705322\n",
      "1.7186505794525146\n",
      "1.7187275886535645\n",
      "1.719135046005249\n",
      "1.7184110879898071\n",
      "1.7163389921188354\n",
      "1.7178864479064941\n",
      "1.7197558879852295\n",
      "1.7209919691085815\n",
      "1.7174315452575684\n",
      "1.7186391353607178\n",
      "1.720169186592102\n",
      "1.7167867422103882\n",
      "1.7175933122634888\n",
      "1.7182081937789917\n",
      "1.7182426452636719\n",
      "1.7189005613327026\n",
      "1.7184877395629883\n",
      "1.7176953554153442\n",
      "1.7176501750946045\n",
      "1.7200870513916016\n",
      "1.718747854232788\n",
      "1.7178313732147217\n",
      "1.7162193059921265\n",
      "1.7180182933807373\n",
      "1.7168225049972534\n",
      "1.7163035869598389\n",
      "1.7168304920196533\n",
      "1.7177356481552124\n",
      "1.717727780342102\n",
      "1.7170263528823853\n",
      "1.7177263498306274\n",
      "1.7181875705718994\n",
      "1.7187305688858032\n",
      "1.7176393270492554\n",
      "1.7171838283538818\n",
      "1.719636082649231\n",
      "1.7181435823440552\n",
      "1.719573974609375\n",
      "1.718932032585144\n",
      "1.719927191734314\n",
      "1.7185194492340088\n",
      "1.7187186479568481\n",
      "1.7187875509262085\n",
      "1.7186944484710693\n",
      "1.7199116945266724\n",
      "1.718351125717163\n",
      "1.7187286615371704\n",
      "1.720521092414856\n",
      "1.7169809341430664\n",
      "1.7187258005142212\n",
      "1.717997670173645\n",
      "1.7184659242630005\n",
      "1.7184007167816162\n",
      "1.7187633514404297\n",
      "1.7193459272384644\n",
      "1.719299554824829\n",
      "1.7168642282485962\n",
      "1.7198034524917603\n",
      "1.7178244590759277\n",
      "1.7182226181030273\n",
      "1.7191983461380005\n",
      "1.7178491353988647\n",
      "1.7175633907318115\n",
      "1.717115044593811\n",
      "1.7185002565383911\n",
      "1.7183839082717896\n",
      "1.7178239822387695\n",
      "1.7191461324691772\n",
      "1.7201734781265259\n",
      "1.7199119329452515\n",
      "1.7186923027038574\n",
      "1.718923568725586\n",
      "1.7172949314117432\n",
      "1.7195152044296265\n",
      "1.7195627689361572\n",
      "1.7171509265899658\n",
      "1.7187559604644775\n",
      "1.7175768613815308\n",
      "1.7182148694992065\n",
      "1.7172755002975464\n",
      "1.7176084518432617\n",
      "1.7170648574829102\n",
      "1.7171112298965454\n",
      "1.7179309129714966\n",
      "1.7187055349349976\n",
      "1.719253659248352\n",
      "1.7184844017028809\n",
      "1.7176107168197632\n",
      "1.718842625617981\n",
      "1.7164673805236816\n",
      "1.71856689453125\n",
      "1.717543125152588\n",
      "1.7178579568862915\n",
      "1.71853506565094\n",
      "1.717965006828308\n",
      "1.7186084985733032\n",
      "1.7209196090698242\n",
      "1.7199124097824097\n",
      "1.7184479236602783\n",
      "1.7173011302947998\n",
      "1.7180854082107544\n",
      "1.7192827463150024\n",
      "1.7179781198501587\n",
      "1.717984676361084\n",
      "1.7187482118606567\n",
      "1.7174689769744873\n",
      "1.7189937829971313\n",
      "1.7194744348526\n",
      "1.721764326095581\n",
      "1.7173861265182495\n",
      "1.7191835641860962\n",
      "1.7195169925689697\n",
      "1.7167141437530518\n",
      "1.7167108058929443\n",
      "1.7181974649429321\n",
      "1.7193315029144287\n",
      "1.717344880104065\n",
      "1.7201275825500488\n",
      "1.7175201177597046\n",
      "1.7184194326400757\n",
      "1.7181029319763184\n",
      "1.7171342372894287\n",
      "1.718900203704834\n",
      "1.7188836336135864\n",
      "1.7183912992477417\n",
      "1.718684196472168\n",
      "1.717816948890686\n",
      "1.718634843826294\n",
      "1.7196598052978516\n",
      "1.720380187034607\n",
      "1.717261791229248\n",
      "1.718749761581421\n",
      "1.7190806865692139\n",
      "1.7184877395629883\n",
      "1.7195953130722046\n",
      "1.718113899230957\n",
      "1.7165546417236328\n",
      "1.7181799411773682\n",
      "1.7212949991226196\n",
      "1.7189422845840454\n",
      "1.7197778224945068\n",
      "1.718840479850769\n",
      "1.7184282541275024\n",
      "1.7185227870941162\n",
      "1.7185817956924438\n",
      "1.7197948694229126\n",
      "1.717842936515808\n",
      "1.7184514999389648\n",
      "1.7200748920440674\n",
      "1.7181148529052734\n",
      "1.7167521715164185\n",
      "1.7177634239196777\n",
      "1.7183326482772827\n",
      "1.7175079584121704\n",
      "1.7190463542938232\n",
      "1.7166333198547363\n",
      "1.7184300422668457\n",
      "1.7188009023666382\n",
      "1.7175631523132324\n",
      "1.7186447381973267\n",
      "1.718228816986084\n",
      "1.7197020053863525\n",
      "1.7184568643569946\n",
      "1.7200212478637695\n",
      "1.7180149555206299\n",
      "1.71800696849823\n",
      "1.7180110216140747\n",
      "1.7186472415924072\n",
      "1.719712495803833\n",
      "1.7177655696868896\n",
      "1.7201265096664429\n",
      "1.7190728187561035\n",
      "1.719738245010376\n",
      "1.719514012336731\n",
      "1.7185450792312622\n",
      "1.71949303150177\n",
      "1.7192946672439575\n",
      "1.7185312509536743\n",
      "1.7184855937957764\n",
      "1.7185630798339844\n",
      "1.7184675931930542\n",
      "1.718687891960144\n",
      "1.7186429500579834\n",
      "1.7194753885269165\n",
      "1.7169737815856934\n",
      "1.7174392938613892\n",
      "1.7185709476470947\n",
      "1.717099666595459\n",
      "1.7190561294555664\n",
      "1.7184438705444336\n",
      "1.7177067995071411\n",
      "1.7190470695495605\n",
      "1.7188313007354736\n",
      "1.717712163925171\n",
      "1.7189339399337769\n",
      "1.718418002128601\n",
      "1.7184287309646606\n",
      "1.718404769897461\n",
      "1.7206294536590576\n",
      "1.719591736793518\n",
      "1.7170054912567139\n",
      "1.7182226181030273\n",
      "1.7190089225769043\n",
      "1.7194006443023682\n",
      "1.7169829607009888\n",
      "1.7183016538619995\n",
      "1.718520998954773\n",
      "1.7180370092391968\n",
      "1.7196838855743408\n",
      "1.7179867029190063\n",
      "1.7180055379867554\n",
      "1.7187983989715576\n",
      "1.7188446521759033\n",
      "1.7199333906173706\n",
      "1.7191230058670044\n",
      "1.7195721864700317\n",
      "1.7186992168426514\n",
      "1.719096302986145\n",
      "1.7179193496704102\n",
      "1.7175064086914062\n",
      "1.719719409942627\n",
      "1.718063235282898\n",
      "1.7178432941436768\n",
      "1.718016266822815\n",
      "1.7168285846710205\n",
      "1.7162271738052368\n",
      "1.7196199893951416\n",
      "1.7177263498306274\n",
      "1.7168912887573242\n",
      "1.7187750339508057\n",
      "1.7177116870880127\n",
      "1.7169581651687622\n",
      "1.7189871072769165\n",
      "1.717495083808899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7179006338119507\n",
      "1.7184644937515259\n",
      "1.7200355529785156\n",
      "1.7184956073760986\n",
      "1.718680500984192\n",
      "1.7181634902954102\n",
      "1.7193546295166016\n",
      "1.7185124158859253\n",
      "1.7190176248550415\n",
      "1.7176787853240967\n",
      "1.717116117477417\n",
      "1.718778133392334\n",
      "1.7183071374893188\n",
      "1.7187509536743164\n",
      "1.717525601387024\n",
      "1.718712568283081\n",
      "1.7175577878952026\n",
      "1.717323899269104\n",
      "1.7177844047546387\n",
      "1.7192816734313965\n",
      "1.717535376548767\n",
      "1.7172904014587402\n",
      "1.720382809638977\n",
      "1.7188389301300049\n",
      "1.7196263074874878\n",
      "1.7190455198287964\n",
      "1.7198591232299805\n",
      "1.7178363800048828\n",
      "1.718355417251587\n",
      "1.7202907800674438\n",
      "1.7178906202316284\n",
      "1.7170093059539795\n",
      "1.7164561748504639\n",
      "1.7189439535140991\n",
      "1.7162063121795654\n",
      "1.7193958759307861\n",
      "1.7169170379638672\n",
      "1.7212682962417603\n",
      "1.7176998853683472\n",
      "1.7187913656234741\n",
      "1.7177644968032837\n",
      "1.7193790674209595\n",
      "1.7187747955322266\n",
      "1.7188218832015991\n",
      "1.7195379734039307\n",
      "1.7167198657989502\n",
      "1.7179781198501587\n",
      "1.7167761325836182\n",
      "1.717245101928711\n",
      "1.7185828685760498\n",
      "1.7208431959152222\n",
      "1.7187063694000244\n",
      "1.7191976308822632\n",
      "1.7186083793640137\n",
      "1.7178505659103394\n",
      "1.716812252998352\n",
      "1.7194174528121948\n",
      "1.718902349472046\n",
      "Looping 1000 times took 3.484421 seconds\n",
      "Result is [1.2317803 1.6187935 1.5227807 ... 2.2077181 2.2996776 1.623233 ]\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "t0 = time.time()\n",
    "for i in range(iters):\n",
    "    rng = torch.rand(vlen).cuda()\n",
    "    print(torch.exp(rng).cuda().mean())\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "-1.0\n",
      "-2.0\n",
      "-1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.],\n",
       "       [-1.,  1.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.linspace(1,-1,3)\n",
    "x=np.linspace(1,3,3)\n",
    "xy=0\n",
    "for i,j in zip(x,y):\n",
    "    xy+=((i-x.mean())*(j-y.mean()))\n",
    "    print(xy)\n",
    "print(xy/(x.std()*y.std())/3)\n",
    "np.correlate(x,y)\n",
    "np.correlate((x-x.mean())/x.std(),(y-y.mean())/y.std())\n",
    "np.corrcoef((x-x.mean())/x.std(),(y-y.mean())/y.std())\n",
    "#np.corrcoef(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100  # size of toy data\n",
    "p = 1    # number of features\n",
    "\n",
    "def build_linear_dataset(N, noise_std=0.1):\n",
    "    X = np.linspace(-6, 6, num=N)\n",
    "    y = 3 * X + 1 + np.random.normal(0, noise_std, size=N)\n",
    "    X, y = X.reshape((N, 1)), y.reshape((N, 1))\n",
    "    X, y = Variable(torch.Tensor(X)), Variable(torch.Tensor(y))\n",
    "    return torch.cat((X, y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f33632e9828>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAAF59JREFUeJzt3X+MZWV9x/H3h2UsE6kslsEuA3TRIoIu7ppxS0Pa6CJC1eBKmlQaCWlNVo0SsJa6aNpqE8O2VKlJjckqKEmpxSAuRlSkgDWais66yy9XikWtO2zZsToRZAO7y7d/3HN3L7P3x7n3nnPPr88rmezMmTt7nzs7z2ef+T7fc44iAjMzq76jih6AmZllw4FuZlYTDnQzs5pwoJuZ1YQD3cysJhzoZmY14UA3M6sJB7qZWU040M3MauLoST7ZCSecEKtXr57kU5qZVd727dt/HhEzgx430UBfvXo18/Pzk3xKM7PKk/TTNI9zycXMrCYc6GZmNeFANzOrCQe6mVlNONDNzGpiol0uo9i2Y4Fr73iYx5b2cdLKaa664Aw2rpstelhmZgNNOr9KHejbdixw9a0PsG//QQAWlvZx9a0PADjUzazUisivUpdcrr3j4UPfjLZ9+w9y7R0PFzQiM7N0isivUgf6Y0v7hjpuZlYWReRXqQP9pJXTQx03MyuLIvJrYKBLOkbSdyXdJ+khSR9Ojn9W0o8l7Uze1mY9uKsuOIPpqRXPOTY9tYKrLjgj66cyM8tUEfmVZlP0aWBDRDwpaQr4lqSvJp+7KiJuyWtw7Y0Dd7mYWdUUkV8DAz0iAngy+XAqeYvcRrTMxnWzDnAzq6RJ51eqtkVJK4DtwO8Cn4iIeyW9C/iIpL8B7gI2R8TT+Q3VPelmVn5F5lSqTdGIOBgRa4GTgfWSXgFcDbwMeDXwQuD93b5W0iZJ85LmFxcXRx5ou6dzYWkfweGezm07Fkb+O83MslR0Tg3V5RIRS8A3gAsjYk+0PA18Bljf42u2RsRcRMzNzAy8PntP7kk3s7IrOqfSdLnMSFqZvD8NvA74oaRVyTEBG4EH8xyoe9LNrOyKzqk0NfRVwI1JHf0o4PMR8WVJd0uaAQTsBN6Z4zg5aeU0C12+Ke5JN7OyKDqnBq7QI+L+iFgXEWdHxCsi4u+S4xsiYk1y7G0R8eSgv2sc7kk3s7IrOqdKfXGuTu5JN7OyKzqn1Gozn4y5ubnwTaLNzIYjaXtEzA16XGVW6N24L93MyqAsWVTZQPe10s2sDMqURaW+2mI/Rfd7mplBubKosoFedL+nmRmUK4sqG+i+VrqZlUGZsqiygV50v6eZGZQriyq7KVp0v6eZGZQri9yHbmZWco3oQ+9Ulj5QM2uGMmZOLQK9TH2gZlZ/Zc2cym6KdipTH6iZ1V9ZM6cWgV6mPlAzq7+yZk4tAr1MfaBmVn9lzZxaBHqZ+kDNrP7Kmjm12BQtUx+omdVfWTOnln3oZWwnMrPqKypbGteH3lbWdiIzq7YqZMvAGrqkYyR9V9J9kh6S9OHk+GmS7pX0iKSbJT0v/+EOVtZ2IjOrtipkS5pN0aeBDRHxSmAtcKGkc4C/B66LiNOBXwJvz2+Y6ZW1ncjMqq0K2TIw0KPlyeTDqeQtgA3ALcnxG4GNuYxwSGVtJzKzaqtCtqRqW5S0QtJOYC9wJ/DfwFJEHEgeshvoWkSStEnSvKT5xcXFLMbcV1nbicys2qqQLakCPSIORsRa4GRgPXBmt4f1+NqtETEXEXMzMzOjjzSljetmuebiNcyunEbA7Mpprrl4TWk2LcysmqqQLUN1uUTEkqRvAOcAKyUdnazSTwYey2F8I9m4bvbQN7ndZvTem3e6hdHMhlalNug0XS4zklYm708DrwN2AfcAf5w87DLgtrwGOap2m9HC0j6Cw21G23YsFD00M6uAqmVImpLLKuAeSfcD3wPujIgvA+8H/kLSj4DfAq7Pb5ijqUKbkZmVV9UyZGDJJSLuB9Z1Of4orXp6aVWhzcjMyqtqGVKLi3P1UoU2IzMrr6plSK0DvQptRmZWXlXLkNpdy6VTWa+IZmbVULUMqeXVFnupUvuRmRWnbFnR2Kst9lKFK6WZWfGqnBW1rqF3qlr7kZkVo8pZ0ZhAr1r7kZkVo8pZ0ZhAr1r7kZkVo8pZ0ZhAr1r7kZkVo8pZ0ZhN0aq1H5lZMaqcFY1qW+xUtrYkMytWmTPBbYt9VLktycyyV5dMaEwNvVOV25LMLHt1yYRGBnqV25LMLHt1yYRGBnqV25LMLHt1yYRGBnqV25LMLHt1yYRGbopWuS3JzLJXl0xobNtipzK3K5lZfqoy99O2Laa5SfQpku6RtEvSQ5KuSI5/SNKCpJ3J2xuyGPikVe0msGaWjTrO/TQ19APA+yLiTOAc4N2Szko+d11ErE3evpLbKHNUl3YlMxtOHed+mptE7wH2JO8/IWkXUL7fSUZUl3YlMxtOHef+UF0uklYD64B7k0PvkXS/pBskHZ/x2CaiLu1KZjacOs791IEu6VjgC8CVEfEr4JPAS4C1tFbwH+3xdZskzUuaX1xczGDI2apLu5KZDaeOcz9VoEuaohXmN0XErQAR8XhEHIyIZ4FPAeu7fW1EbI2IuYiYm5mZyWrcmdm4bpZrLl7D7MppBMyunOaai9eUcqfbzLJTx7k/sIYuScD1wK6I+FjH8VVJfR3gLcCD+QwxfxvXzR76R2y3Mb335p2lbmMys9FUpVVxFGlOLDoXuBR4QNLO5NgHgEskrQUC+AnwjlxGOEF1ueKamXVX9zmepsvlW4C6fKqSbYr99GtjqsM/tlnT1X2ON/JaLr3UsY3JzA6r+xx3oHeoYxuTmR1W9znuQO9QxzYmMzus7nO8kVdb7GX5FdeOm55CgvfevJNr73i4VrvhZk3S2dly3PQUx0wdxdJT+xvZ5dIo7RbGuu+GmzXF8rm8tG8/01MruO5P1tZuLrvk0kMdL9xj1kRNmssO9B7qvhtu1hRNmssO9B7qvhtu1hRNmssO9B7qvhtu1hRNmsveFO3BHS9m1daUzpZODvQ+3PFiVk1N6mzp5JJLCk3aJTerg6bOWQd6Ck3aJTerg6bOWQd6Ck3aJTerg6bOWQd6Ck3aJTerg6bOWUXExJ5sbm4u5ufnJ/Z8WVq+Yy5R+x1zsypZfiei175shnt+uFiLOxNJ2h4RcwMf50AfzvLdc2j9z1/1exGaVVnd52XaQHfJZUhN3T03KzPPy5aBgS7pFEn3SNol6SFJVyTHXyjpTkmPJH8en/9wi9fU3XOzMvO8bEmzQj8AvC8izgTOAd4t6SxgM3BXRJwO3JV8XHtN3T03KzPPy5aBgR4ReyLi+8n7TwC7gFngzcCNycNuBDbmNcgyaeruuVmZeV62DHXqv6TVwDrgXuBFEbEHWqEv6cTMR1dCvsaLWXk08Xot/aQOdEnHAl8AroyIX0lK+3WbgE0Ap5566ihjLB1f48WseE29Xks/qbpcJE3RCvObIuLW5PDjklYln18F7O32tRGxNSLmImJuZmYmizGXhnfWzYrj+XekNF0uAq4HdkXExzo+9SXgsuT9y4Dbsh9euXln3aw4nn9HSrNCPxe4FNggaWfy9gZgC3C+pEeA85OPG8U762bF8fw7Upoul29FhCLi7IhYm7x9JSL+LyLOi4jTkz9/MYkBl4l31s2K4/l3JN/gYgzueDGbPHe29OZAH5M7Xswmx50t/flaLhnxjrtZ/jzP+nOgZ8Q77mb58zzrz4GeEe+4m+XP86w/B3pGvONulj/Ps/68KZoRd7yY5cedLek40DPkjhez7LmzJT2XXHLgnXiz7Hg+pedAz4F34s2y4/mUngM9B96JN8uO51N6DvQcdNuJnzpKPPXMAU7bfDvnbrmbbTsWChqdWTVs27HAuVvuZmFpH8vvvuDOlu68KZqDbh0vv37mAL98aj/gTVKzQZZvhAag5M9Zd7b05EDPSbvjBeDcLXeztG//cz7f3tTxD6XZkbpthLbD/NubNxQzqApwyWUCvKljNhzPmdE40CfAmzpmw/GcGY0DfQJ8urLZcDxnRuMa+gT4sgBm6fgU//E40CfElwUw68+n+I9vYMlF0g2S9kp6sOPYhyQtLLtptKXg05jNuvPcGF+aGvpngQu7HL+u86bR2Q6rvrx7b9ad58b4BgZ6RHwT+MUExtII3r03685zY3zjdLm8R9L9SUnm+MxGVHPddu9Fq5buSwJYE/kU/+yMGuifBF4CrAX2AB/t9UBJmyTNS5pfXFwc8enqY+O6Wa65eA2zyaqjfTozHN4gdahbU7Q3QheSskr7FH9onRV6zcVrvCE6hJECPSIej4iDEfEs8ClgfZ/Hbo2IuYiYm5mZGXWctbJx3Szf3ryB2ZXTh8K8zZtA1iSDTvF3mA9npECXtKrjw7cAD/Z6rPXmTSBrOs+BbA3sQ5f0OeA1wAmSdgN/C7xG0lpa/5n+BHhHjmOsrZNWTh/6VXP5cbMm8BzIVpoul0siYlVETEXEyRFxfURcGhFrIuLsiLgoIvZMYrB14w1SaypvhObDZ4oWqPOSAO0f7OUbpJ2PM6sDX+s8P744V8G8QWpN443Q/DjQS8KbQ9YU/lnPjwO9JHyWnDWFf9bz40AvCW+QWt15IzR/3hQtCW+QWp15I3QyvEIvEW+QWl15I3QyHOgl5E0jqxv/TE+GA72Eem0OBbiebpXSrpsv/42zzRuh2XKgl1C3DdI2X5HRqmL5lRSX80Zo9hzoJbT8ErvLuZ5uVdCtbt7mS+Pmw10uJdW+qfRpm2/v+uuqa49Wdr1+RgV8e/OGyQ6mIbxCLzmfhGFV5Z/dyXOgl5xPOLKq8QlExXHJpeR8wpFViU8gKpZX6BXgE46sKnwCUbEc6BXikzOs7PwzWiwHeoX4hCMrK59AVA4DA13SDZL2Snqw49gLJd0p6ZHkz+PzHaaBTziycvIJROWRZoX+WeDCZcc2A3dFxOnAXcnHljOfcGRl5BOIyiPNTaK/Cfxi2eE3Azcm798IbMx4XNZDe4N0eTtYm2uVNmmDTiBymE/OqDX0F0XEHoDkzxOzG5Kl4Xq6Fc118/LJfVNU0iZJ85LmFxcX8366xnA93Yrkunk5jRroj0taBZD8ubfXAyNia0TMRcTczMzMiE9ny7mebkVy3bycRg30LwGXJe9fBtyWzXBsGK6nW1FcNy+nNG2LnwP+EzhD0m5Jbwe2AOdLegQ4P/nYCuJ6uk2K6+blNvBaLhFxSY9PnZfxWGxEV11wxnOun9HJ13uxrCy/TstyrpsXz2eK1oDr6TYJrpuXn6+2WBO+IYblzTesKD+v0GvG9XTLUrtmftrm2zlK3bffXTcvDwd6zbg/3bLS2WsewME48nc/183LxYFeM66nW1Z61cxXSAjXzcvINfQacj3dstDr5+TZCH685Y0THo2l4RV6jbmebqNwr3l1OdBrzPV0G5av0VJtDvQacz3dhuVe82pzDb3mXE+3YbjXvNq8Qm8I19OtH9fN68GB3hCup1svrpvXhwO9IVxPt15cN68PB3qDDLp++sLSPpdfGqRdZum1Mve1zavHgd5A/eqhLr80w6AyC7huXkUO9AbqV08Hl1+aoF+ZBVw3ryoHegMNqqeDyy91NajMAq6bV5n70Buq3Z/eb3L7bkf1MuiOQ9AKc/ebV5dX6A3n8ktzuMxSf2Ot0CX9BHgCOAgciIi5LAZlk9NeeV97x8N9V+qnbb6dk1ZOc9UFZ3i1XjHbdiz0/feF1src/7bVl0XJ5bUR8fMM/h4rSJryS+ASTBW5zNIsLrnYIYPKL+ASTNW4zNIs4wZ6AF+XtF3Spm4PkLRJ0ryk+cXFxTGfzvLU2f3S6+QjcAdMFbibpZkUXe4TmPqLpZMi4jFJJwJ3ApdHxDd7PX5ubi7m5+dHfj6brEGBMD21woFQQi6z1I+k7Wn2KMdaoUfEY8mfe4EvAuvH+fusXNwBU00uszTXyIEu6fmSfrP9PvB64MGsBmbF8wlI1eIyi43T5fIi4IuS2n/Pv0bE1zIZlZWGT0CqBpdZDMZYoUfEoxHxyuTt5RHxkSwHZuWSpvxy5c07vVqfsPaq/Mqbd7rMYj7139JJcwISeLU+SWlW5eCThprEfeiWWvt66v1q6uDN0kkZtPkJh8ssDvNmcKDb0NKcgOTN0vyk2fwEl1mayCUXG5rLL8VxmcX6GevEomH5xKL6ccBMRpoLbIFP9qqrtCcWeYVuY/FqPX/+T9PS8grdMpOmrgsOnrTSrsrBPeZ1N5FT/806pdksBd+IOo00N3Fu8+antXmFbpkaZlUJXq0v5++fdeMauhWifamAtHVf19YPS/s9A29+WndeoVtuvNpMx98nG8QrdCucV+uDeVVuWfIK3SbCq9Dn8vfDhpF2he5At4kaZkUqWvc4rEOYtQP8saV9HDc9xa+fOcD+g4PnnlflBg50K7FhV6dQzXDvfJ3t8Q+jSq/V8uVAt9IbZrXeqczhPm6Ig1fldiQHulXCKKv1TmUI9yxCvK2M/0lZ8RzoVimjrtY7TR0ljj3maJae2s9JOQdjliEOXpVbfxMJdEkXAh8HVgCfjogt/R7vQLd+sg7J9t+xcnoKiZGCfvlmpgS/fGr/2OOb5H8+Vn25B7qkFcB/AecDu4HvAZdExA96fY0D3dLKOtw7dQv647q8n0Vwd3tel1VsWJM4sWg98KOIeDR5wn8D3gz0DHSztNonJUH24d7++qV9+w8d6/V+Vr8lOMRtEsYJ9FngZx0f7wZ+b7zhmB0pz3DPg0PcijJOoKvLsSPml6RNwCaAU089dYynM+se7sOerJMHh7iVwTiBvhs4pePjk4HHlj8oIrYCW6FVQx/j+cyeozPcYfKrd4e4lc04gf494HRJpwELwFuBP81kVGYj6Ld6H2eTM4tuGbNJGDnQI+KApPcAd9BqW7whIh7KbGRmY1i+em/rFvS9ulwc3FY1PrHIzKzkfE9RM7OGcaCbmdWEA93MrCYc6GZmNeFANzOriYl2uUhaBH464pefAPw8w+EUya+lfOryOsCvpYzGfR2/ExEzgx400UAfh6T5NG07VeDXUj51eR3g11JGk3odLrmYmdWEA93MrCaqFOhbix5AhvxayqcurwP8WspoIq+jMjV0MzPrr0ordDMz66NygS7pckkPS3pI0j8UPZ5xSfpLSSHphKLHMgpJ10r6oaT7JX1R0sqixzQsSRcmP1M/krS56PGMStIpku6RtCuZH1cUPaZxSFohaYekLxc9lnFIWinplmSe7JL0+3k9V6UCXdJrad239OyIeDnwjwUPaSySTqF1k+3/KXosY7gTeEVEnE3rpuFXFzyeoSQ3O/8E8EfAWcAlks4qdlQjOwC8LyLOBM4B3l3h1wJwBbCr6EFk4OPA1yLiZcAryfE1VSrQgXcBWyLiaYCI2FvweMZ1HfBXlPPWmKlExNcj4kDy4Xdo3bmqSg7d7DwingHaNzuvnIjYExHfT95/glZwVPJC7pJOBt4IfLrosYxD0guAPwSuB4iIZyJiKa/nq1qgvxT4A0n3SvoPSa8uekCjknQRsBAR9xU9lgz9OfDVogcxpG43O69kCHaStBpYB9xb7EhG9k+0FjvPFj2QMb0YWAQ+k5SPPi3p+Xk92Ti3oMuFpH8HfrvLpz5Ia7zH0/p18tXA5yW9OEraqjPgtXwAeP1kRzSafq8jIm5LHvNBWr/y3zTJsWUg1c3Oq0TSscAXgCsj4ldFj2dYkt4E7I2I7ZJeU/R4xnQ08Crg8oi4V9LHgc3AX+f1ZKUSEa/r9TlJ7wJuTQL8u5KepXWNhMVJjW8YvV6LpDXAacB9kqBVpvi+pPUR8b8THGIq/f5NACRdBrwJOK+s/7n2kepm51UhaYpWmN8UEbcWPZ4RnQtcJOkNwDHACyT9S0S8reBxjWI3sDsi2r8p3UIr0HNRtZLLNmADgKSXAs+jghfuiYgHIuLEiFgdEatp/aO/qoxhPoikC4H3AxdFxFNFj2cEh252Lul5tG52/qWCxzQStVYH1wO7IuJjRY9nVBFxdUScnMyNtwJ3VzTMSeb0zySdkRw6D/hBXs9XuhX6ADcAN0h6EHgGuKyCK8K6+WfgN4A7k982vhMR7yx2SOnV7Gbn5wKXAg9I2pkc+0BEfKXAMRlcDtyULBgeBf4sryfymaJmZjVRtZKLmZn14EA3M6sJB7qZWU040M3MasKBbmZWEw50M7OacKCbmdWEA93MrCb+H+GqhOUbKLXDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-6, 6, num=N)\n",
    "#x,y,z=build_linear_dataset(100)\n",
    "y=x*x\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(p, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "regression_model = RegressionModel(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0050] loss: 8839.3018\n",
      "[iteration 0100] loss: 6130.5488\n",
      "[iteration 0150] loss: 4124.8853\n",
      "[iteration 0200] loss: 2689.6694\n",
      "[iteration 0250] loss: 1701.4731\n",
      "[iteration 0300] loss: 1049.1094\n",
      "[iteration 0350] loss: 637.4866\n",
      "[iteration 0400] loss: 389.8778\n",
      "[iteration 0450] loss: 248.1486\n",
      "[iteration 0500] loss: 171.0564\n",
      "Learned parameters:\n",
      "linear.weight: 2.767\n",
      "linear.bias: 1.181\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optim = torch.optim.Adam(regression_model.parameters(), lr=0.01)\n",
    "num_iterations = 500\n",
    "\n",
    "def main():\n",
    "    data = build_linear_dataset(N, p)\n",
    "    x_data = data[:, :-1]\n",
    "    y_data = data[:, -1]\n",
    "    for j in range(num_iterations):\n",
    "        # run the model forward on the data\n",
    "        y_pred = regression_model(x_data)\n",
    "        # calculate the mse loss\n",
    "        loss = loss_fn(y_pred, y_data)\n",
    "        # initialize gradients to zero\n",
    "        optim.zero_grad()\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # take a gradient step\n",
    "        optim.step()\n",
    "        if (j + 1) % 50 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.data[0]))\n",
    "    # Inspect learned parameters\n",
    "    print(\"Learned parameters:\")\n",
    "    for name, param in regression_model.named_parameters():\n",
    "        print(\"%s: %.3f\" % (name, param.data.numpy()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n",
      " 0.9346  0.8221  0.9160  ...   0.4609  0.9975  0.5149\n",
      " 0.3294  0.9540  0.6797  ...   0.6439  0.4402  0.2785\n",
      " 0.4150  0.8610  0.4438  ...   0.9160  0.5206  0.3503\n",
      "          ...             ⋱             ...          \n",
      " 0.8968  0.2835  0.1347  ...   0.9755  0.4420  0.5171\n",
      " 0.5906  0.5801  0.3682  ...   0.2328  0.7493  0.2015\n",
      " 0.7859  0.9240  0.7231  ...   0.0510  0.0459  0.2803\n",
      "[torch.cuda.FloatTensor of size 97x102 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.rand(100^5,100^2).cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.0'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#theano.__version__\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set plotting style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d221203fef74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"patch.force_edgecolor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m sns.set_style('whitegrid',\n\u001b[1;32m      3\u001b[0m               {'axes.grid': True,\n\u001b[1;32m      4\u001b[0m                \u001b[0;34m'grid.linestyle'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mu'--'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                \u001b[0;34m'axes.edgecolor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'0.1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "sns.set_style('whitegrid',\n",
    "              {'axes.grid': True,\n",
    "               'grid.linestyle': u'--',\n",
    "               'axes.edgecolor': '0.1',\n",
    "               'axes.labelcolor': '0',\n",
    "               'axes.labelsize': 15,\n",
    "               'axes.titlesize': 15,\n",
    "               'legend.fontsize': 15,\n",
    "               'xtick.labelsize': 15,\n",
    "               'ytick.labelsize': 15,\n",
    "               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook_1_CNN_Model_Training_with_Hyperparameter_Search.ipynb\r\n",
      "Notebook_2_CNN_Predictions_of_Random_UTR_HIS3_data.ipynb\r\n",
      "Notebook_3_CNN_Predictions_of_Native_UTR_HIS3_data.ipynb\r\n",
      "Notebook_4_Generating_Model_Directed_Evolution_of_UTRs_From_100_Native_and_Random_UTRs.ipynb\r\n",
      "saved_model.t7\r\n",
      "utr_pytorch.ipynb\r\n",
      "Yeast_Pytorch.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the various directories that you might care about in the future are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = '../Data/'\n",
    "results_dir = '../Results/'\n",
    "figures_dir = '../Figures/'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.mkdir(results_dir)\n",
    "    \n",
    "if not os.path.exists(figures_dir):\n",
    "    os.mkdir(figures_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a directory to save the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Random_UTR_CNN'\n",
    "model_params_dir = '../Results/{0}.Hyperparam.Opt/'.format(model_name)\n",
    "\n",
    "if not os.path.exists(model_params_dir):\n",
    "    os.mkdir(model_params_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvolvedUTRs\n",
      "Random_UTR_CNN.Hyperparam.Opt\n",
      "Random_UTR_CNN_Hyperparams.tar.gz\n",
      "161222_ip_ugi_band_PSMs.txt\n",
      "170523_1_EColi_170524165739-(2)_PeptideGroups.txt\n",
      "170523_1_EColi_170524165739-(2)_ProteinGroups.txt\n",
      "170523_1_EColi_170524165739-(2)_Proteins.txt\n",
      "170523_1_EColi_170524165739-(2)_ResultStatistics.txt\n",
      "2017---Deep-learning-yeast-UTRs\n",
      "aifMsms.txt\n",
      "allPeptides.txt\n",
      "check.txt\n",
      "CMakeLists.txt\n",
      "Combo.txt\n",
      "Comet-chunk.ipynb\n",
      "Comet.ipynb\n",
      "CoMMpass_IA8b.txt\n",
      "CoMMpass_IA8b.txtcoxPHsortNoLog2.txt\n",
      "CoMMpass_IA8b.txtcoxPHsort.txt\n",
      "CoMMpass_IA8b.txtcoxPH.txt\n",
      "Compass_Survival_per.txt\n",
      "Compass_Survival.txt\n",
      "Copy of 170214 total proteome lymphom 0p05 SILAC pluss LFQ (2).txt\n",
      "Copy of Strainssamples_shuffled_final_150617.txt\n",
      "Copy of SuperSILACpTtestImp.txt\n",
      "Copy of toptags_DE5 (2).txt\n",
      "Copy of toptags_DE5 (3).txt\n",
      "Deamidation (NQ)Sites.txt\n",
      "ECCO 2017 Breath Test.txt\n",
      "EcoliMascotLog2SSttestPB.txt\n",
      "Elasticsearch.ipynb\n",
      "evidence.txt\n",
      "fastakey.txt\n",
      "GroupsMascot.txt\n",
      "Groups.txt\n",
      "keras.json\n",
      "Keyfile_pegfam_annot.txt\n",
      "LFQttestPBs0p10.txt\n",
      "LFQvalues.txt\n",
      "LFQwoImpttestPBs0p10.txt\n",
      "libraryMatch.txt\n",
      "license.txt\n",
      "LICENSE.txt\n",
      "log2LFQttestBHcorrection50percentchangeAnnotatedPvalue5percent.txt\n",
      "log2LFQttestBHcorrection50percentchange.txt\n",
      "log2LFQttestBHcorrection.txt\n",
      "Log2proteinGroupsTtestCLExcelCL2KEGG.txt\n",
      "Log2proteinGroupsTtestCLExcelCL2.txt\n",
      "Log2proteinGroupsTtestCLsheetKEGG.txt\n",
      "Log2SSttestGroups.txt\n",
      "matchedFeatures.txt\n",
      "modificationSpecificPeptides.txt\n",
      "ms3Scans.txt\n",
      "msmsScans.txt\n",
      "msms.txt\n",
      "msScans.txt\n",
      "MyelomaProjectB1B2combineTTpermcorr.txt\n",
      "mzRange.txt\n",
      "newbooks.xml\n",
      "NotFoundInAnnotation.txt\n",
      "oldnotebooks\n",
      "Oxidation (M)Sites.txt\n",
      "parameters.txt\n",
      "PeptideList.txt\n",
      "peptides.txt\n",
      "pg16665.txt\n",
      "pg30601.txt\n",
      "Promec - R.ipynb\n",
      "proteinGroups - Copy.txt\n",
      "proteinGroupsImpPermANOVAfiltered.txt\n",
      "proteinGroupsImpPermANOVA.txt\n",
      "proteinGroupsSILACmedianInverted.txt\n",
      "proteinGroups.txt\n",
      "RankedFtr100.txt\n",
      "RankedFtr100Vals.txt\n",
      "RankedFtr.txt\n",
      "README.txt\n",
      "search-for-xlinks.qvalues..txt\n",
      "Sel81.txt\n",
      "spark-warehouse\n",
      "SSttestBHcorrection.txt\n",
      "SSttestPBs0p10.txt\n",
      "summary.txt\n",
      "totall_identifications.txt\n",
      "Trp of SuperSILACpTtestImp.txt\n",
      "Xml-Copy1.ipynb\n",
      "Xml-Copy2.ipynb\n",
      "Xml-Copy3.ipynb\n",
      "Xml-Copy4.ipynb\n",
      "Xml.ipynb\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls ../Results/\n",
    "\n",
    "ls ../..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our cleaned up data.\n",
    "\n",
    "The csv should be tab-separated. The read counts are log2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clone</th>\n",
       "      <th>RFP</th>\n",
       "      <th>DNASeq</th>\n",
       "      <th>Pos1</th>\n",
       "      <th>Pos2</th>\n",
       "      <th>Pos3</th>\n",
       "      <th>Pos4</th>\n",
       "      <th>Pos5</th>\n",
       "      <th>Pos6</th>\n",
       "      <th>Pos7</th>\n",
       "      <th>...</th>\n",
       "      <th>Pos203</th>\n",
       "      <th>Pos204</th>\n",
       "      <th>Pos205</th>\n",
       "      <th>Pos206</th>\n",
       "      <th>Pos207</th>\n",
       "      <th>Pos208</th>\n",
       "      <th>Pos209</th>\n",
       "      <th>Pos210</th>\n",
       "      <th>Pos211</th>\n",
       "      <th>pred_growth_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E6</td>\n",
       "      <td>2246</td>\n",
       "      <td>TCTCCTCATTTCTATTGAGTCATCCTAGCATTAGAGTTATTTGTCA...</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>5310.665527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F4</td>\n",
       "      <td>3878</td>\n",
       "      <td>CTTAGCGTGTTCGCGTGTCGAGTAGTGTAATGATAAATATTTTTAA...</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>5325.514648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A4</td>\n",
       "      <td>4854</td>\n",
       "      <td>ATTTCTGCGATCCGGTTAGGAAATAACTTTCCTGAATGTGAAGGTG...</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>5292.278809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G2</td>\n",
       "      <td>5191</td>\n",
       "      <td>TTGACCCTATCTTGGTCTATTAAAGTGCAGGCACGTACATGTTTCT...</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>5322.292969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D5</td>\n",
       "      <td>5203</td>\n",
       "      <td>GTGTCCTTTAACCAGATCGGCGTCATGTGCGTGTGAACGGAGTTCT...</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>5304.737305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 215 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Clone   RFP                                             DNASeq Pos1 Pos2  \\\n",
       "0    E6  2246  TCTCCTCATTTCTATTGAGTCATCCTAGCATTAGAGTTATTTGTCA...    T    C   \n",
       "1    F4  3878  CTTAGCGTGTTCGCGTGTCGAGTAGTGTAATGATAAATATTTTTAA...    C    T   \n",
       "2    A4  4854  ATTTCTGCGATCCGGTTAGGAAATAACTTTCCTGAATGTGAAGGTG...    A    T   \n",
       "3    G2  5191  TTGACCCTATCTTGGTCTATTAAAGTGCAGGCACGTACATGTTTCT...    T    T   \n",
       "4    D5  5203  GTGTCCTTTAACCAGATCGGCGTCATGTGCGTGTGAACGGAGTTCT...    G    T   \n",
       "\n",
       "  Pos3 Pos4 Pos5 Pos6 Pos7       ...        Pos203 Pos204 Pos205 Pos206  \\\n",
       "0    T    C    C    T    C       ...             A      G      T      T   \n",
       "1    T    A    G    C    G       ...             A      G      G      T   \n",
       "2    T    T    C    T    G       ...             A      G      C      G   \n",
       "3    G    A    C    C    C       ...             A      G      G      C   \n",
       "4    G    T    C    C    T       ...             A      G      A      G   \n",
       "\n",
       "  Pos207 Pos208 Pos209 Pos210 Pos211 pred_growth_rate  \n",
       "0      G      C      A      T      A      5310.665527  \n",
       "1      G      G      T      T      T      5325.514648  \n",
       "2      T      C      G      T      A      5292.278809  \n",
       "3      G      A      T      C      T      5322.292969  \n",
       "4      C      G      T      C      G      5304.737305  \n",
       "\n",
       "[5 rows x 215 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#data = pd.read_csv(data_dir + 'Random_UTRs.csv.gz')\n",
    "#data = pd.read_csv(\"https://github.com/animesh/2017---Deep-learning-yeast-UTRs/raw/master/Data/Random_UTRs.csv.gz\")\n",
    "#data = pd.read_csv(\"https://github.com/animesh/2017---Deep-learning-yeast-UTRs/raw/master/Data/posw.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1., 10.,  9.,  8., 13., 22., 15.,  3.,  6.]),\n",
       " array([11.13314221, 11.60700167, 12.08086113, 12.5547206 , 13.02858006,\n",
       "        13.50243952, 13.97629898, 14.45015844, 14.9240179 , 15.39787736,\n",
       "        15.87173682]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "plt.hist(np.log2(df['expression']))\n",
    "#data.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.rename(columns={\"DNASeq\":\"seq_column\",\"RFP\":\"expression\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding of the sequences.\n",
    "\n",
    "i.e. we're converting the sequences from being represented as a 50 character string of bases to a 4x50 matrix of 1's and 0's, with each row corresponding to a base and every column a position in the UTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of UTRs\n",
    "# X = one hot encoding matrix\n",
    "# Y = growth rates\n",
    "\n",
    "def one_hot_encoding(df, seq_column, expression):\n",
    "    \n",
    "    bases = ['A','C','G','T']\n",
    "    base_dict = dict(zip(bases,range(4))) # {'A' : 0, 'C' : 1, 'G' : 2, 'T' : 3}\n",
    "\n",
    "    n = len(df)\n",
    "    \n",
    "    # length of the UTR sequence\n",
    "    # we also add 10 empty spaces to either side\n",
    "    total_width = df[seq_column].str.len().max() + 20\n",
    "    \n",
    "    # initialize an empty numpy ndarray of the appropriate size\n",
    "    X = np.zeros((n, 1, 4, total_width))\n",
    "    \n",
    "    # an array with the sequences that we will one-hot encode\n",
    "    seqs = df[seq_column].values\n",
    "    \n",
    "    # loop through the array of sequences to create an array that keras will actually read\n",
    "    for i in range(n):\n",
    "        seq = seqs[i]\n",
    "        \n",
    "        # loop through each individual sequence, from the 5' to 3' end\n",
    "        for b in range(len(seq)):\n",
    "            # this will assign a 1 to the appropriate base and position for this UTR sequence\n",
    "            X[i, 0, base_dict[seq[b]], int(b + round((total_width - len(seq))/2.))] = 1.\n",
    "    \n",
    "        # keep track of where we are\n",
    "        if (i%10)==0:\n",
    "            print(i),\n",
    "        \n",
    "    X = X.astype(theano.config.floatX)\n",
    "    Y = np.asarray(df[expression].values,\n",
    "                   dtype = theano.config.floatX)[:, np.newaxis]\n",
    "    \n",
    "    return X, Y, total_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "X, Y, total_width = one_hot_encoding(df, 'seq_column', 'expression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " array([14.82970972, 14.92970972, 15.02970972, 15.12970972, 15.22970972,\n",
       "        15.32970972, 15.42970972, 15.52970972, 15.62970972, 15.72970972,\n",
       "        15.82970972]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log2(Y))\n",
    "plt.hist(np.sqrt(total_width))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into test and training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have more reads for a given UTR at the outset, we can be more confident that we have made an accurate measurement. For this reason, we use those UTRs with the most reads to test our model on, because these should have the least experimental noise. We hold out the UTRs that fall in top 5% of reads at the first time point as our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9 19 29 39 49 59 69  8  0 10 11 12 13 14 15 16 17 18  1 20 21 22 23 24\n",
      " 25 26 27 28  2 30 31 32 33 34 35 36 37 38  3 40 41 42 43 44 45 46 47 48\n",
      "  4 50 51 52 53 54 55 56 57 58  5 60 61 62 63 64 65 66 67 68  6 70 71 72\n",
      " 73 74 75 76 77 78  7] [79 80 81 82 83 84 85 86 87]\n"
     ]
    }
   ],
   "source": [
    "# a sorted numpy array of UTR indexes, from least reads to most reads\n",
    "sorted_inds = df.sort_values('expression').index.values\n",
    "train_inds = sorted_inds[:int(0.9*len(sorted_inds))]\n",
    "test_inds = sorted_inds[int(0.9*len(sorted_inds)):] \n",
    "seed = 0.1\n",
    "random.shuffle(train_inds, lambda :seed)\n",
    "print(train_inds,test_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search\n",
    "\n",
    "Before training the model, we perform a hyperparameter search to narrow down which model architecture to use. Of course, we do a fair amount of narrowing ourselves by selecting which architectures are available for the search.\n",
    "\n",
    "The dictionary 'hyperparams' has the same values as the 'space' object, 'space' is just the form that's compatible with hyperopt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {'conv_width' : [9, 13, 17, 25],\n",
    "               'conv_filters' : [32, 64, 128, 256],\n",
    "               'conv_layers' : [2, 3, 4],\n",
    "               'dense_layers' : [1, 2],\n",
    "               'conv_dropout' : [None, 0.15],\n",
    "               #'dense_dropout' : [None, 0.1, 0.25, 0.5],\n",
    "               'dense_dropout' : [None, 0.0025, 0.005, 0.0075],\n",
    "               'dense_units' : [32, 64, 128, 256]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {   'conv_width': hp.choice('conv_width', [9, 13, 17, 25]),\n",
    "            'conv_filters': hp.choice('conv_filters', [32, 64, 128, 256]),\n",
    "            'conv_layers': hp.choice('conv_layers', [2, 3, 4]),\n",
    "            'dense_layers': hp.choice('dense_layers', [1, 2]),\n",
    "            'conv_dropout': hp.choice('conv_dropout',  [None, 0.15]),\n",
    "            #'dense_dropout': hp.choice('dense_dropout', [None, 0.1, 0.25, 0.5]),\n",
    "            'dense_dropout': hp.choice('dense_dropout', [None, 0.026, 0.051, 0.076]),\n",
    "            'dense_units': hp.choice('dense_units', [32, 64, 128, 256]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a function to build the model\n",
    "\n",
    "- Note: we reuse this same function lower down after we've decide on a model architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(params['conv_filters'],\n",
    "                            4,\n",
    "                            params['conv_width'],\n",
    "                            border_mode = 'valid',\n",
    "                            input_shape = (1, 4, total_width),\n",
    "                            activation = 'relu'))\n",
    "    \n",
    "    # add dropout at the convolutional layers if appropriate\n",
    "    if params['conv_dropout']:\n",
    "        model.add(Dropout(p = params['conv_dropout']))\n",
    "    \n",
    "    # add the appropriate number of additional convolutional layers, along with dropout\n",
    "    for i in range(params['conv_layers'] - 1):\n",
    "        model.add(Convolution2D(params['conv_filters'],\n",
    "                                1,\n",
    "                                params['conv_width'],\n",
    "                                border_mode = 'same',\n",
    "                                activation = 'relu'))\n",
    "        \n",
    "        if params['conv_dropout']:\n",
    "            model.add(Dropout(params['conv_dropout']))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # add the appropriate number of dense layers and dropout\n",
    "    for i in range(params['dense_layers']):\n",
    "        model.add(Dense(output_dim = params['dense_units'],\n",
    "                        activation = 'relu'))\n",
    "        \n",
    "        if params['dense_dropout']:\n",
    "            model.add(Dropout(p = params['dense_dropout']))\n",
    "    \n",
    "    # add the output layer, since we want to predict the \"growth rate\" we only want a single \n",
    "    # number, hence the single dimension\n",
    "    model.add(Dense(output_dim = 1))\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss = 'mean_squared_error',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['mean_squared_error'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also need a function that builds and fits the model, which we can pass to the hyperparameter search.\n",
    "\n",
    "- It also returns some information regarding overfitting, etc.\n",
    "- note that the data is included inside the function, I'm not passing it to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_nn(params):\n",
    "    \n",
    "    model = create_model(params)\n",
    "    \n",
    "    # track model overfitting\n",
    "    earlyStopping = keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "                                                  patience = 1,\n",
    "                                                  verbose = 0,\n",
    "                                                  mode = 'auto')\n",
    "    history = keras.callbacks.History()\n",
    "    \n",
    "    # keep track of where we are while the code in this cell is running\n",
    "    global n\n",
    "    print(\"\\n\"), n\n",
    "    n+=1\n",
    "    print(params)\n",
    "    \n",
    "    # fit the model\n",
    "    # note that I'm not passing the data to this function, I've just included it here (i.e. I've\n",
    "    # included X and Y)\n",
    "    model.fit(X[train_inds],\n",
    "              Y[train_inds],\n",
    "              validation_split = 0.2,\n",
    "              callbacks = [earlyStopping, history],\n",
    "              verbose = 0,\n",
    "              nb_epoch = 100)\n",
    "    \n",
    "    print('MSE:',earlyStopping.best)\n",
    "    return {'loss': earlyStopping.best, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually perform the hyperparameter search.\n",
    "\n",
    "A note here, there're random elements in keras and hypfmin that I don't understand how to control, so I haven't been able to set a seed that will allow you to obtain exactly the same results for the hyperparameter search that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 256, 'conv_layers': 2, 'conv_width': 25, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 66074040.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 2, 'conv_width': 25, 'dense_dropout': 0.076, 'dense_layers': 1, 'dense_units': 32}\n",
      "MSE: 86126648.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 128}\n",
      "MSE: 52584860.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 32, 'conv_layers': 3, 'conv_width': 9, 'dense_dropout': 0.051, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 84484448.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 32, 'conv_layers': 3, 'conv_width': 17, 'dense_dropout': 0.076, 'dense_layers': 2, 'dense_units': 256}\n",
      "MSE: 74256312.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': None, 'dense_layers': 2, 'dense_units': 256}\n",
      "MSE: 53051804.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 2, 'conv_width': 9, 'dense_dropout': 0.051, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 79856960.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 32, 'conv_layers': 3, 'conv_width': 25, 'dense_dropout': 0.076, 'dense_layers': 2, 'dense_units': 32}\n",
      "MSE: 78524656.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 256, 'conv_layers': 3, 'conv_width': 9, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 32}\n",
      "MSE: 71562784.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 2, 'conv_width': 17, 'dense_dropout': 0.051, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 83056288.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 2, 'conv_width': 25, 'dense_dropout': 0.026, 'dense_layers': 2, 'dense_units': 256}\n",
      "MSE: 83399088.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': 0.051, 'dense_layers': 2, 'dense_units': 256}\n",
      "MSE: 199906032.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 64, 'conv_layers': 2, 'conv_width': 13, 'dense_dropout': 0.051, 'dense_layers': 2, 'dense_units': 64}\n",
      "MSE: 85041144.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 32, 'conv_layers': 3, 'conv_width': 17, 'dense_dropout': 0.051, 'dense_layers': 2, 'dense_units': 32}\n",
      "MSE: 79448768.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 256, 'conv_layers': 3, 'conv_width': 17, 'dense_dropout': 0.051, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 58533084.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 256, 'conv_layers': 2, 'conv_width': 13, 'dense_dropout': 0.076, 'dense_layers': 2, 'dense_units': 256}\n",
      "MSE: 71265392.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 64, 'conv_layers': 2, 'conv_width': 25, 'dense_dropout': 0.051, 'dense_layers': 2, 'dense_units': 64}\n",
      "MSE: 79205856.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 256, 'conv_layers': 4, 'conv_width': 9, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 128}\n",
      "MSE: 55394460.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 64, 'conv_layers': 2, 'conv_width': 13, 'dense_dropout': 0.026, 'dense_layers': 2, 'dense_units': 128}\n",
      "MSE: 86424128.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 64, 'conv_layers': 2, 'conv_width': 9, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 101044184.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 128}\n",
      "MSE: 140451648.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': None, 'dense_layers': 2, 'dense_units': 128}\n",
      "MSE: 100219552.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 128}\n",
      "MSE: 95427872.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': None, 'dense_layers': 2, 'dense_units': 128}\n",
      "MSE: 69387032.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 128}\n",
      "MSE: 94335232.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 13, 'dense_dropout': None, 'dense_layers': 2, 'dense_units': 128}\n",
      "MSE: 76667584.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': None, 'dense_layers': 2, 'dense_units': 128}\n",
      "MSE: 161789296.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 133393104.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 154089040.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': 0.026, 'dense_layers': 2, 'dense_units': 32}\n",
      "MSE: 98823928.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 32, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 86288528.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 64, 'conv_layers': 4, 'conv_width': 13, 'dense_dropout': 0.076, 'dense_layers': 1, 'dense_units': 128}\n",
      "MSE: 65506540.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 256, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 2, 'dense_units': 256}\n",
      "MSE: 141033040.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 32}\n",
      "MSE: 106330544.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 32, 'conv_layers': 4, 'conv_width': 9, 'dense_dropout': 0.076, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 76680192.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 3, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 2, 'dense_units': 64}\n",
      "MSE: 65613160.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 32, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 76705600.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 3, 'conv_width': 9, 'dense_dropout': 0.076, 'dense_layers': 2, 'dense_units': 32}\n",
      "MSE: 82145856.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 256, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 128}\n",
      "MSE: 139756544.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 3, 'conv_width': 25, 'dense_dropout': None, 'dense_layers': 2, 'dense_units': 256}\n",
      "MSE: 64383372.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 32, 'conv_layers': 4, 'conv_width': 13, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 75357664.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.076, 'dense_layers': 2, 'dense_units': 32}\n",
      "MSE: 102575888.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 64, 'conv_layers': 3, 'conv_width': 25, 'dense_dropout': 0.051, 'dense_layers': 2, 'dense_units': 64}\n",
      "MSE: 68829232.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 2, 'conv_width': 9, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 83964384.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 256, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': None, 'dense_layers': 2, 'dense_units': 128}\n",
      "MSE: 164653552.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 3, 'conv_width': 25, 'dense_dropout': 0.051, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 85624688.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 64, 'conv_layers': 2, 'conv_width': 13, 'dense_dropout': 0.076, 'dense_layers': 2, 'dense_units': 32}\n",
      "MSE: 99636160.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 32, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 128}\n",
      "MSE: 78783632.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 256, 'conv_layers': 2, 'conv_width': 9, 'dense_dropout': None, 'dense_layers': 2, 'dense_units': 64}\n",
      "MSE: 74409120.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': 0.051, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 143933152.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 64, 'conv_layers': 3, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 2, 'dense_units': 128}\n",
      "MSE: 74251352.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 13, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 256}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 100428784.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 2, 'conv_width': 25, 'dense_dropout': 0.026, 'dense_layers': 2, 'dense_units': 128}\n",
      "MSE: 78070504.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 256, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.051, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 56975436.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 32, 'conv_layers': 4, 'conv_width': 9, 'dense_dropout': None, 'dense_layers': 2, 'dense_units': 128}\n",
      "MSE: 78554512.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': 0.076, 'dense_layers': 1, 'dense_units': 32}\n",
      "MSE: 54714208.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 2, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 2, 'dense_units': 256}\n",
      "MSE: 71872216.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 64, 'conv_layers': 3, 'conv_width': 13, 'dense_dropout': None, 'dense_layers': 2, 'dense_units': 128}\n",
      "MSE: 76251792.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 156422112.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 54136252.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 256, 'conv_layers': 4, 'conv_width': 9, 'dense_dropout': 0.051, 'dense_layers': 2, 'dense_units': 128}\n",
      "MSE: 124520672.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 32, 'conv_layers': 2, 'conv_width': 25, 'dense_dropout': 0.076, 'dense_layers': 1, 'dense_units': 32}\n",
      "MSE: 93995160.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': None, 'dense_layers': 2, 'dense_units': 256}\n",
      "MSE: 113163952.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 64, 'conv_layers': 3, 'conv_width': 13, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 128}\n",
      "MSE: 70370464.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': None, 'dense_layers': 2, 'dense_units': 256}\n",
      "MSE: 121999712.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 120025632.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 102082464.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 55360752.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 125048584.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 53312164.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 114306632.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 101338896.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': None, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 76432912.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 53103544.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 128}\n",
      "MSE: 108754032.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 32, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 32}\n",
      "MSE: 83469848.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 256, 'conv_layers': 4, 'conv_width': 9, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 94210560.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 2, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 2, 'dense_units': 128}\n",
      "MSE: 72565376.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 64, 'conv_layers': 3, 'conv_width': 13, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 74047496.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': 0.026, 'dense_layers': 2, 'dense_units': 256}\n",
      "MSE: 175790016.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 128}\n",
      "MSE: 121483840.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 32, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 74721312.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 256, 'conv_layers': 4, 'conv_width': 9, 'dense_dropout': 0.051, 'dense_layers': 2, 'dense_units': 32}\n",
      "MSE: 95059712.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 2, 'conv_width': 17, 'dense_dropout': 0.076, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 80195872.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 64, 'conv_layers': 3, 'conv_width': 25, 'dense_dropout': 0.026, 'dense_layers': 2, 'dense_units': 128}\n",
      "MSE: 66204956.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 13, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 82897344.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 2, 'dense_units': 64}\n",
      "MSE: 77651984.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.051, 'dense_layers': 1, 'dense_units': 128}\n",
      "MSE: 56436600.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 256, 'conv_layers': 2, 'conv_width': 25, 'dense_dropout': 0.076, 'dense_layers': 2, 'dense_units': 256}\n",
      "MSE: 74874480.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 3, 'conv_width': 9, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 32}\n",
      "MSE: 79276352.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 32, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 128}\n",
      "MSE: 68685824.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 64, 'conv_layers': 4, 'conv_width': 13, 'dense_dropout': 0.051, 'dense_layers': 2, 'dense_units': 64}\n",
      "MSE: 76029344.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': 0.076, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 79199960.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 2, 'dense_units': 128}\n",
      "MSE: 90760352.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 256, 'conv_layers': 2, 'conv_width': 25, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 81957888.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 32, 'conv_layers': 3, 'conv_width': 9, 'dense_dropout': 0.026, 'dense_layers': 2, 'dense_units': 32}\n",
      "MSE: 88608936.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 17, 'dense_dropout': 0.051, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 65842408.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 4, 'conv_width': 25, 'dense_dropout': 0.076, 'dense_layers': 2, 'dense_units': 128}\n",
      "MSE: 71391264.0\n",
      "\n",
      "\n",
      "{'conv_dropout': None, 'conv_filters': 64, 'conv_layers': 4, 'conv_width': 13, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 64}\n",
      "MSE: 89678464.0\n",
      "\n",
      "\n",
      "{'conv_dropout': 0.15, 'conv_filters': 128, 'conv_layers': 2, 'conv_width': 17, 'dense_dropout': 0.026, 'dense_layers': 1, 'dense_units': 256}\n",
      "MSE: 76778784.0\n",
      "best:  {'conv_dropout': 0, 'conv_filters': 2, 'conv_layers': 2, 'conv_width': 3, 'dense_dropout': 1, 'dense_layers': 0, 'dense_units': 2}\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "n = 0\n",
    "\n",
    "trials = Trials()\n",
    "best = hypfmin(f_nn, space,algo = tpe.suggest,max_evals = 100,trials = trials)\n",
    "\n",
    "print('best: ',best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_params_dir + 'hyperparam_test.pkl', 'wb') as f:\n",
    "#with open(model_params_dir + 'hyperparam_test.pkl', 'w') as f:\n",
    "    pickle.dump(trials.trials, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and take a look a the winning architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params = {}\n",
    "\n",
    "for p in best:\n",
    "    opt_params[p] = hyperparams[p][best[p]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv_dropout': None,\n",
       " 'conv_filters': 128,\n",
       " 'conv_layers': 4,\n",
       " 'conv_width': 25,\n",
       " 'dense_dropout': 0.0025,\n",
       " 'dense_layers': 1,\n",
       " 'dense_units': 128}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                              patience = 0,\n",
    "                                              verbose = 0,\n",
    "                                              mode = 'auto')\n",
    "\n",
    "history = keras.callbacks.History()\n",
    "\n",
    "modelcheckpoint = keras.callbacks.ModelCheckpoint(model_params_dir + 'model_weights.hdf5',\n",
    "                                                  monitor = 'val_loss',\n",
    "                                                  verbose = 0,\n",
    "                                                  save_best_only = True,\n",
    "                                                  mode = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71 samples, validate on 8 samples\n",
      "Epoch 1/100\n",
      "71/71 [==============================] - 0s - loss: 237496848.6761 - mean_squared_error: 237496859.0423 - val_loss: 686337984.0000 - val_mean_squared_error: 686337920.0000\n",
      "Epoch 2/100\n",
      "71/71 [==============================] - 0s - loss: 173247682.9859 - mean_squared_error: 173247690.1972 - val_loss: 155538064.0000 - val_mean_squared_error: 155538048.0000\n",
      "Epoch 3/100\n",
      "71/71 [==============================] - 0s - loss: 268325327.2113 - mean_squared_error: 268325325.4085 - val_loss: 478208544.0000 - val_mean_squared_error: 478208544.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f335b74a1d0>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X[train_inds],\n",
    "          Y[train_inds],\n",
    "          validation_split = 0.1,\n",
    "          callbacks = [earlyStopping,\n",
    "                       history,\n",
    "                       modelcheckpoint],\n",
    "          verbose=1,\n",
    "          nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "open(model_params_dir + 'model_arch.json', 'w').write(json_string)\n",
    "model.save_weights(model_params_dir + 'model_weightsN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model to predict the growth rates from our library of 5' UTR sequences\n",
    "\n",
    "- we do this on the entire library because we want to compare the fits of the test and training data.\n",
    "    - you would generally expect to maybe do a little better on the training data. However, since we use the highest quality data for our test set -- the values that we're most confident about -- it's not maybe not that surprising that our predictions are more accurate on our test set (see results two cells down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/88 [=========>....................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04645662733173683\n",
      "0.007959088095697509\n"
     ]
    }
   ],
   "source": [
    "# R^2 value for our predictions on the training set\n",
    "print(scipy.stats.pearsonr(Y[train_inds].flatten(),Y_pred[train_inds].flatten())[0]**2)\n",
    "# and on the test set\n",
    "print(scipy.stats.pearsonr(Y[test_inds].flatten(),Y_pred[test_inds].flatten())[0]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure the model architecture and parameters are saved correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparam_test.pkl  model_weights.h5\t model_weightsN.h5\r\n",
      "model_arch.json      model_weights.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "!ls {model_params_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 6 layers into a model with 5 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-2eceac393d79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_params_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'model_arch.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_params_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'model_weights.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   2706\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   2758\u001b[0m                                  \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m                                  \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2760\u001b[0;31m                                  str(len(flattened_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m   2761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2762\u001b[0m             \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 6 layers into a model with 5 layers."
     ]
    }
   ],
   "source": [
    "model = keras.models.model_from_json(open(model_params_dir + 'model_arch.json').read())\n",
    "model.load_weights(model_params_dir + 'model_weights.hdf5')\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/88 [=========>....................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04645662733173683\n",
      "0.007959088095697509\n"
     ]
    }
   ],
   "source": [
    "# R^2 value for our predictions on the training set\n",
    "print(scipy.stats.pearsonr(Y[train_inds].flatten(),\n",
    "                           Y_pred[train_inds].flatten())[0]**2)\n",
    "\n",
    "# and on the test set\n",
    "print(scipy.stats.pearsonr(Y[test_inds].flatten(),\n",
    "                           Y_pred[test_inds].flatten())[0]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6448: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(-3,1.25,'CNN predictions vs. test set')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data\n",
    "x = Y_pred[test_inds].flatten()\n",
    "y = Y[test_inds].flatten()\n",
    "\n",
    "# calculate R^2\n",
    "r2 = scipy.stats.pearsonr(x, y)[0]**2\n",
    "\n",
    "\n",
    "g = sns.jointplot(x,\n",
    "                  y,\n",
    "                  stat_func = None,\n",
    "                  kind = 'scatter',\n",
    "                  s = 5,\n",
    "                  alpha = 0.1,\n",
    "                  size = 5)\n",
    "\n",
    "g.ax_joint.set_xlabel('Predicted log$_2$ Growth Rate')\n",
    "g.ax_joint.set_ylabel('Measured log$_2$ Growth Rate')\n",
    "\n",
    "\n",
    "\n",
    "text = \"R$^2$ = {:0.2}\".format(r2)\n",
    "plt.annotate(text, xy=(-5.5, 0.95), xycoords='axes fraction')\n",
    "\n",
    "plt.title(\"CNN predictions vs. test set\", x = -3, y = 1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data and predictions to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pred_growth_rate'] = Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(data_dir + 'Random_UTRs_with_predictionsN.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WIP torch implementation via mikael.huss@scilifelab.se / https://github.com/hussius/deeplearning-biology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNADataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, seq_len):\n",
    "        self.data = df\n",
    "        self.bases = ['A','C','G','T']\n",
    "        self.base_dict = dict(zip(self.bases,range(4))) # {'A' : 0, 'C' : 1, 'G' : 2, 'T' : 3}\n",
    "        self.total_width = seq_len + 20\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.data.shape[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data.iloc[idx].UTR\n",
    "        X = np.zeros((1, 4, self.total_width))\n",
    "        y = self.data.iloc[idx].growth_rate\n",
    "        for b in range(len(seq)):\n",
    "            # this will assign a 1 to the appropriate base and position for this UTR sequence\n",
    "            X[0, self.base_dict[seq[b]], int(b + round((self.total_width - len(seq))/2.))] = 1.\n",
    "        return(seq, X, y)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=128, kernel_size=(4, 13))\n",
    "        self.dropout = nn.Dropout(p=0.15)\n",
    "        self.conv2 = nn.Conv2d(128, 128, (1,13))\n",
    "        self.fc1 = nn.Linear(128 * 1 * 34, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 128 * 1 * 34)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "net = net.cuda() # to run it on GPU, if available\n",
    "\n",
    "# Loss function etc.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from ipywidgets import IntProgress\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d (1, 128, kernel_size=(4, 13), stride=(1, 1))\n",
       "  (dropout): Dropout(p=0.15)\n",
       "  (conv2): Conv2d (128, 128, kernel_size=(1, 13), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=4352, out_features=64)\n",
       "  (fc2): Linear(in_features=64, out_features=1)\n",
       ")"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = int(0.9*len(train_inds))\n",
    "val_inds = train_inds[val_idx:]\n",
    "train_inds = train_inds[:val_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DNADataset(df.iloc[train_inds], seq_len=50)\n",
    "val_data = DNADataset(df.iloc[val_inds], seq_len=50)\n",
    "test_data = DNADataset(df.iloc[test_inds], seq_len=50)\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=32,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "val_data_loader = DataLoader(val_data, batch_size=32) # Validate everything in one batch?!\n",
    "#seq_val, X_val, y_val = next(iter(val_data_loader))\n",
    "\n",
    "test_data_loader = DataLoader(test_data, batch_size=32) # Validate everything in one batch?!\n",
    "#seq_test, X_test, y_test = next(iter(test_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampled_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-e4528da42911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msampled_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sampled_batch' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_loader\n",
    "sampled_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-154-1d8d12150ba9>\", line 13, in __getitem__\n    seq = self.data.iloc[idx].UTR\n  File \"/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\", line 3614, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'Series' object has no attribute 'UTR'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-58fd7411f18f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrowth_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrowth_rate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    953\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-154-1d8d12150ba9>\", line 13, in __getitem__\n    seq = self.data.iloc[idx].UTR\n  File \"/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\", line 3614, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'Series' object has no attribute 'UTR'\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for i_batch, sampled_batch in enumerate(tqdm(train_data_loader)):\n",
    "        sequence, transformed_sequence, growth_rate = sampled_batch\n",
    "        inputs, labels = Variable(transformed_sequence.float().cuda()), Variable(growth_rate.float().cuda())\n",
    "        optimizer.zero_grad()\n",
    "        net.train()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    error = 0\n",
    "    total = 0\n",
    "    net.eval()\n",
    "    for batch in tqdm(val_data_loader):\n",
    "      v_seq, X_v, y_v = batch\n",
    "      v_pred = net(Variable(X_v.float().cuda()))\n",
    "      total += y_v.size(0)\n",
    "      raw_error = v_pred[:,0].data - y_v.float().cuda()\n",
    "      error += (raw_error**2).sum()\n",
    "      avg_mse = error / float(total)\n",
    "      tqdm.write('avg_mse: %.3f' % avg_mse)\n",
    "#torch.save(net, 'saved_model.t7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 0\n",
    "total = 0\n",
    "for batch in tqdm(val_data_loader):\n",
    "                v_seq, X_v, y_v = batch\n",
    "                v_pred = net(Variable(X_v.float().cuda()))\n",
    "                total += y_v.size(0)\n",
    "                raw_error = v_pred[:,0].data - y_v.float().cuda()\n",
    "                error += (raw_error**2).sum()\n",
    "\n",
    "avg_mse = error / float(total)\n",
    "\n",
    "print(\"Validation error: {}\".format(avg_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = [test_data[i][2] for i in range(len(test_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-5b033932e1ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactuals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "r2 = scipy.stats.pearsonr(preds, actuals)[0]**2\n",
    "print(r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
