{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export CUDA_LIB=\"/usr/local/cuda-9.2/lib64\"\n",
    "export CUDA_ROOT=\"/usr/local/cuda-9.2/bin\"\n",
    "export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:$CUDA_LIB\n",
    "export PATH=${PATH}:$CUDA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 1: /home/notebook/.theanorc: Permission denied\n",
      "bash: line 2: [global]: command not found\n",
      "bash: line 3: device: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "$HOME/.theanorc\n",
    "[global]\n",
    "device = cuda\n",
    "#floatX = float32\n",
    "force_device=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.optimizers import Adam\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import hyperopt.fmin as hypfmin\n",
    "import keras\n",
    "import theano\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theano.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#theano-cache purge\n",
    "#rm -rf ~/.theano\n",
    "print(theano.config.device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras==1.2.2 theano==0.9 hyperopt==0.1 scipy==0.19.0 pandas==0.19.2  seaborn==0.8 numpy==1.12.1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pygpu --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install theano --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install  --upgrade hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: keras in /opt/conda/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from keras) (1.0.6)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from keras) (0.19.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/lib/python3.6/site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/lib/python3.6/site-packages (from keras) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from keras) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras) (1.16.2)\n",
      "\u001b[33mYou are using pip version 19.0.2, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install keras --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set plotting style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "sns.set_style('whitegrid',\n",
    "              {'axes.grid': True,\n",
    "               'grid.linestyle': u'--',\n",
    "               'axes.edgecolor': '0.1',\n",
    "               'axes.labelcolor': '0',\n",
    "               'axes.labelsize': 15,\n",
    "               'axes.titlesize': 15,\n",
    "               'legend.fontsize': 15,\n",
    "               'xtick.labelsize': 15,\n",
    "               'ytick.labelsize': 15,\n",
    "               })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the various directories that you might care about in the future are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../Data/'\n",
    "results_dir = '../Results/'\n",
    "figures_dir = '../Figures/'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.mkdir(results_dir)\n",
    "    \n",
    "if not os.path.exists(figures_dir):\n",
    "    os.mkdir(figures_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a directory to save the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'Random_UTR_CNN'\n",
    "model_params_dir = '../Results/{0}.Hyperparam.Opt/'.format(model_name)\n",
    "\n",
    "if not os.path.exists(model_params_dir):\n",
    "    os.mkdir(model_params_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our cleaned up data.\n",
    "\n",
    "The csv should be tab-separated. The read counts are log2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_dir + 'Random_UTRs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding of the sequences.\n",
    "\n",
    "i.e. we're converting the sequences from being represented as a 50 character string of bases to a 4x50 matrix of 1's and 0's, with each row corresponding to a base and every column a position in the UTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encoding of UTRs\n",
    "# X = one hot encoding matrix\n",
    "# Y = growth rates\n",
    "\n",
    "def one_hot_encoding(df, seq_column, expression):\n",
    "    \n",
    "    bases = ['A','C','G','T']\n",
    "    base_dict = dict(zip(bases,range(4))) # {'A' : 0, 'C' : 1, 'G' : 2, 'T' : 3}\n",
    "\n",
    "    n = len(df)\n",
    "    \n",
    "    # length of the UTR sequence\n",
    "    # we also add 10 empty spaces to either side\n",
    "    total_width = df[seq_column].str.len().max() + 20\n",
    "    \n",
    "    # initialize an empty numpy ndarray of the appropriate size\n",
    "    X = np.zeros((n, 1, 4, total_width))\n",
    "    \n",
    "    # an array with the sequences that we will one-hot encode\n",
    "    seqs = df[seq_column].values\n",
    "    \n",
    "    # loop through the array of sequences to create an array that keras will actually read\n",
    "    for i in range(n):\n",
    "        seq = seqs[i]\n",
    "        \n",
    "        # loop through each individual sequence, from the 5' to 3' end\n",
    "        for b in range(len(seq)):\n",
    "            # this will assign a 1 to the appropriate base and position for this UTR sequence\n",
    "            X[i, 0, base_dict[seq[b]], int(b + round((total_width - len(seq))/2.))] = 1.\n",
    "    \n",
    "        # keep track of where we are\n",
    "        if (i%10000)==0:\n",
    "            print(i),\n",
    "        \n",
    "    X = X.astype(theano.config.floatX)\n",
    "    Y = np.asarray(df[expression].values,\n",
    "                   dtype = theano.config.floatX)[:, np.newaxis]\n",
    "    \n",
    "    return X, Y, total_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n"
     ]
    }
   ],
   "source": [
    "X, Y, total_width = one_hot_encoding(data, 'UTR', 'growth_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into test and training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have more reads for a given UTR at the outset, we can be more confident that we have made an accurate measurement. For this reason, we use those UTRs with the most reads to test our model on, because these should have the least experimental noise. We hold out the UTRs that fall in top 5% of reads at the first time point as our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a sorted numpy array of UTR indexes, from least reads to most reads\n",
    "sorted_inds = data.sort_values('t0').index.values\n",
    "\n",
    "\n",
    "train_inds = sorted_inds[:int(0.95*len(sorted_inds))] # 95% of the data as the training set\n",
    "\n",
    "\n",
    "test_inds = sorted_inds[int(0.95*len(sorted_inds)):] # UTRs with most reads at time point 0 as the test set\n",
    "\n",
    "# set the seed before randomly shuffling the data\n",
    "seed = 0.5\n",
    "random.shuffle(train_inds, lambda :seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search\n",
    "\n",
    "Before training the model, we perform a hyperparameter search to narrow down which model architecture to use. Of course, we do a fair amount of narrowing ourselves by selecting which architectures are available for the search.\n",
    "\n",
    "The dictionary 'hyperparams' has the same values as the 'space' object, 'space' is just the form that's compatible with hyperopt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {'conv_width' : [9, 13, 17, 25],\n",
    "               'conv_filters' : [32, 64, 128, 256],\n",
    "               'conv_layers' : [2, 3, 4],\n",
    "               'dense_layers' : [1, 2],\n",
    "               'conv_dropout' : [None, 0.15],\n",
    "               'dense_dropout' : [None, 0.1, 0.25, 0.5],\n",
    "               'dense_units' : [32, 64, 128, 256]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "space = {   'conv_width': hp.choice('conv_width', [9, 13, 17, 25]),\n",
    "            'conv_filters': hp.choice('conv_filters', [32, 64, 128, 256]),\n",
    "            'conv_layers': hp.choice('conv_layers', [2, 3, 4]),\n",
    "            'dense_layers': hp.choice('dense_layers', [1, 2]),\n",
    "            'conv_dropout': hp.choice('conv_dropout',  [None, 0.15]),\n",
    "            'dense_dropout': hp.choice('dense_dropout', [None, 0.1, 0.25, 0.5]),\n",
    "            'dense_units': hp.choice('dense_units', [32, 64, 128, 256]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a function to build the model\n",
    "\n",
    "- Note: we reuse this same function lower down after we've decide on a model architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(params['conv_filters'],\n",
    "                            4,\n",
    "                            params['conv_width'],\n",
    "                            border_mode = 'valid',\n",
    "                            input_shape = (1, 4, total_width),\n",
    "                            activation = 'relu'))\n",
    "    \n",
    "    # add dropout at the convolutional layers if appropriate\n",
    "    if params['conv_dropout']:\n",
    "        model.add(Dropout(p = params['conv_dropout']))\n",
    "    \n",
    "    # add the appropriate number of additional convolutional layers, along with dropout\n",
    "    for i in range(params['conv_layers'] - 1):\n",
    "        model.add(Convolution2D(params['conv_filters'],\n",
    "                                1,\n",
    "                                params['conv_width'],\n",
    "                                border_mode = 'same',\n",
    "                                activation = 'relu'))\n",
    "        \n",
    "        if params['conv_dropout']:\n",
    "            model.add(Dropout(params['conv_dropout']))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # add the appropriate number of dense layers and dropout\n",
    "    for i in range(params['dense_layers']):\n",
    "        model.add(Dense(output_dim = params['dense_units'],\n",
    "                        activation = 'relu'))\n",
    "        \n",
    "        if params['dense_dropout']:\n",
    "            model.add(Dropout(p = params['dense_dropout']))\n",
    "    \n",
    "    # add the output layer, since we want to predict the \"growth rate\" we only want a single \n",
    "    # number, hence the single dimension\n",
    "    model.add(Dense(output_dim = 1))\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss = 'mean_squared_error',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['mean_squared_error'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also need a function that builds and fits the model, which we can pass to the hyperparameter search.\n",
    "\n",
    "- It also returns some information regarding overfitting, etc.\n",
    "- note that the data is included inside the function, I'm not passing it to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_nn(params):\n",
    "    \n",
    "    model = create_model(params)\n",
    "    \n",
    "    # track model overfitting\n",
    "    earlyStopping = keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "                                                  patience = 1,\n",
    "                                                  verbose = 0,\n",
    "                                                  mode = 'auto')\n",
    "    history = keras.callbacks.History()\n",
    "    \n",
    "    # keep track of where we are while the code in this cell is running\n",
    "    global n\n",
    "    print(\"\\n\"), n\n",
    "    n+=1\n",
    "    print(params)\n",
    "    \n",
    "    # fit the model\n",
    "    # note that I'm not passing the data to this function, I've just included it here (i.e. I've\n",
    "    # included X and Y)\n",
    "    model.fit(X[train_inds],\n",
    "              Y[train_inds],\n",
    "              validation_split = 0.2,\n",
    "              callbacks = [earlyStopping, history],\n",
    "              verbose = 0,\n",
    "              nb_epoch = 100)\n",
    "    \n",
    "    print('MSE:',earlyStopping.best)\n",
    "    return {'loss': earlyStopping.best, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/keras-team/keras/issues/3945#issuecomment-281312732\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "#for ordering error karas 1->2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually perform the hyperparameter search.\n",
    "\n",
    "A note here, there're random elements in keras and hypfmin that I don't understand how to control, so I haven't been able to set a seed that will allow you to obtain exactly the same results for the hyperparameter search that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (4, 13), input_shape=(1, 4, 70), activation=\"relu\", padding=\"valid\")`\n",
      "  \n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 13), activation=\"relu\", padding=\"same\")`\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.5)`\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1)`\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv_dropout': None, 'conv_filters': 32, 'conv_layers': 2, 'conv_width': 13, 'dense_dropout': 0.5, 'dense_layers': 1, 'dense_units': 128}\n",
      "  0%|          | 0/50 [00:06<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n",
      "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:\n",
      "0.6191979255062479\n",
      "  2%|▏         | 1/50 [1:14:36<60:56:11, 4476.97s/it, best loss: 0.6191979255062479]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (4, 13), input_shape=(1, 4, 70), activation=\"relu\", padding=\"valid\")`\n",
      "  \n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 13), activation=\"relu\", padding=\"same\")`\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 13), activation=\"relu\", padding=\"same\")`\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 13), activation=\"relu\", padding=\"same\")`\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=32)`\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1)`\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv_dropout': None, 'conv_filters': 256, 'conv_layers': 4, 'conv_width': 13, 'dense_dropout': 0.1, 'dense_layers': 1, 'dense_units': 32}\n",
      "  2%|▏         | 1/50 [1:14:39<60:56:11, 4476.97s/it, best loss: 0.6191979255062479]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "\n",
    "trials = Trials()\n",
    "best = hypfmin(f_nn, space,\n",
    "               algo = tpe.suggest,\n",
    "               max_evals = 50,\n",
    "               trials = trials)\n",
    "print('best: ')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(model_params_dir + 'hyperparam_test.pkl', 'w') as f:\n",
    "    pickle.dump(trials.trials, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and take a look a the winning architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt_params = {}\n",
    "\n",
    "for p in best:\n",
    "    opt_params[p] = hyperparams[p][best[p]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model(opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                              patience = 0,\n",
    "                                              verbose = 0,\n",
    "                                              mode = 'auto')\n",
    "\n",
    "history = keras.callbacks.History()\n",
    "\n",
    "modelcheckpoint = keras.callbacks.ModelCheckpoint(model_params_dir + 'model_weights.hdf5',\n",
    "                                                  monitor = 'val_loss',\n",
    "                                                  verbose = 0,\n",
    "                                                  save_best_only = True,\n",
    "                                                  mode = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X[train_inds],\n",
    "          Y[train_inds],\n",
    "          validation_split = 0.1,\n",
    "          callbacks = [earlyStopping,\n",
    "                       history,\n",
    "                       modelcheckpoint],\n",
    "          verbose=1,\n",
    "          nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "open(model_params_dir + 'model_arch.json', 'w').write(json_string)\n",
    "model.save_weights(model_params_dir + 'model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model to predict the growth rates from our library of 5' UTR sequences\n",
    "\n",
    "- we do this on the entire library because we want to compare the fits of the test and training data.\n",
    "    - you would generally expect to maybe do a little better on the training data. However, since we use the highest quality data for our test set -- the values that we're most confident about -- it's not maybe not that surprising that our predictions are more accurate on our test set (see results two cells down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# R^2 value for our predictions on the training set\n",
    "print scipy.stats.pearsonr(Y[train_inds].flatten(),\n",
    "                           Y_pred[train_inds].flatten())[0]**2\n",
    "\n",
    "# and on the test set\n",
    "print scipy.stats.pearsonr(Y[test_inds].flatten(),\n",
    "                           Y_pred[test_inds].flatten())[0]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure the model architecture and parameters are saved correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {model_params_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.model_from_json(open(model_params_dir + 'model_arch.json').read())\n",
    "model.load_weights(model_params_dir + 'model_weights.hdf5')\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 value for our predictions on the training set\n",
    "print scipy.stats.pearsonr(Y[train_inds].flatten(),\n",
    "                           Y_pred[train_inds].flatten())[0]**2\n",
    "\n",
    "# and on the test set\n",
    "print scipy.stats.pearsonr(Y[test_inds].flatten(),\n",
    "                           Y_pred[test_inds].flatten())[0]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "x = Y_pred[test_inds].flatten()\n",
    "y = Y[test_inds].flatten()\n",
    "\n",
    "# calculate R^2\n",
    "r2 = scipy.stats.pearsonr(x, y)[0]**2\n",
    "\n",
    "\n",
    "g = sns.jointplot(x,\n",
    "                  y,\n",
    "                  stat_func = None,\n",
    "                  kind = 'scatter',\n",
    "                  s = 5,\n",
    "                  alpha = 0.1,\n",
    "                  size = 5)\n",
    "\n",
    "g.ax_joint.set_xlabel('Predicted log$_2$ Growth Rate')\n",
    "g.ax_joint.set_ylabel('Measured log$_2$ Growth Rate')\n",
    "\n",
    "\n",
    "text = \"R$^2$ = {:0.2}\".format(r2)\n",
    "plt.annotate(text, xy=(-5.5, 0.95), xycoords='axes fraction')\n",
    "\n",
    "plt.title(\"CNN predictions vs. test set\", x = -3, y = 1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data and predictions to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['pred_growth_rate'] = Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv(data_dir + 'Random_UTRs_with_predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
